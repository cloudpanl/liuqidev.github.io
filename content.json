{"meta":{"title":"WizardLQ’s | 魔法师の小茶馆","subtitle":"Keep moving, never give up. | 锲而不舍，金石可镂.","description":"远不止魔法......","author":"liuqidev","url":"https://www.blankspace.cn"},"pages":[{"title":"404 Not Found","date":"2018-08-04T05:55:12.000Z","updated":"2018-08-06T07:40:12.006Z","comments":true,"path":"404.html","permalink":"https://www.blankspace.cn/404.html","excerpt":"","text":""},{"title":"projects","date":"2018-08-09T07:56:23.000Z","updated":"2018-08-09T07:56:51.194Z","comments":true,"path":"projects/index.html","permalink":"https://www.blankspace.cn/projects/index.html","excerpt":"","text":""},{"title":"about","date":"2018-08-09T13:17:50.000Z","updated":"2018-08-09T13:17:50.369Z","comments":true,"path":"about/index.html","permalink":"https://www.blankspace.cn/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-09T13:16:34.000Z","updated":"2018-08-09T13:16:51.810Z","comments":true,"path":"categories/index.html","permalink":"https://www.blankspace.cn/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Activation function","slug":"activation-function","date":"2018-08-13T13:24:57.000Z","updated":"2018-08-13T13:29:12.487Z","comments":true,"path":"2018/08/13/activation-function/","link":"","permalink":"https://www.blankspace.cn/2018/08/13/activation-function/","excerpt":"Activation function（激活函数），是一类非线性函数，与之对应的就是线性函数。线性函数就是一条直线，形式化表示就是： y=mx+b$m$是直线的斜率，$y$通常是预测的值，$x$是输入的特征值，$b$是$y$轴的截距。 由于现实世界的复杂性，输入特征和预测之间的关系通常不是简单的线性关系，换言之就是非线性关系。然后就发现和一些非线性函数，满足一定得性质（这个得看人家的论文），通过非线性函数多层的组合，可以来拟合非常复杂的非线性函数。 更过关于激活函数的介绍，这里. TensorFlow中更多激活函数，Activation Function Binary step f(x) = \\begin{cases} 0, & \\text{for x $\\lt$ 0} \\\\ 1, & \\text{for x $\\geq$ 0} \\end{cases}阶跃函数是理想的激活函数，但是不可导。 下面对几种常见的激活函数进行可视化。 ReLu(rectified linear unit, ReLu) f(x) = \\begin{cases} 0, & \\text{for x $\\lt$ 0} \\\\ x, & \\text{for x $\\geq$ 0} \\end{cases}Logistic(Sigmoid or Soft step) f(x)=\\sigma{(x)} = \\frac{1}{1+e^{-x}}TanH f(x)= \\tanh{(x)}= \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}Softplus f(x)= \\ln{(1+e^x)}Softmax f_{i}(x)= \\frac{e^x_{i}}{\\sum_{j=1}^{J}e^{e^i}}1234567891011121314151617181920212223242526272829303132333435363738import tensorflow as tf # machine learning framworkimport numpy as np # scientific computationimport matplotlib.pyplot as plt # data visualizationwith tf.Graph().as_default(), tf.Session() as sess: x_data = np.linspace(-6, 6, 256) y_relu = tf.nn.relu(x_data) y_sigmoid = tf.nn.sigmoid(x_data) y_tanh = tf.nn.tanh(x_data) y_softplus = tf.nn.softplus(x_data) y_softmax = tf.nn.softmax(x_data) # softmax is a special kind of activation function, it is about probablity y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax = sess.run([y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax]) # plot fig = plt.figure(1, figsize=(8, 6)) plt.subplot(221) plt.plot(x_data, y_relu, c='red', label='relu') plt.ylim(-1, 6) plt.legend(loc='best') plt.subplot(222) plt.plot(x_data, y_sigmoid, c='green', label='sigmoid') plt.ylim(-0.3, 1.5) plt.legend(loc='best') plt.subplot(223) plt.plot(x_data, y_tanh, c='blue', label='tanh') plt.ylim(-1.2, 1.2) plt.legend(loc='best') plt.subplot(224) plt.plot(x_data, y_softplus, c='yellow', label='softplus') plt.ylim(-0.3, 6) plt.legend(loc='best') plt.show()","text":"Activation function（激活函数），是一类非线性函数，与之对应的就是线性函数。线性函数就是一条直线，形式化表示就是： y=mx+b$m$是直线的斜率，$y$通常是预测的值，$x$是输入的特征值，$b$是$y$轴的截距。 由于现实世界的复杂性，输入特征和预测之间的关系通常不是简单的线性关系，换言之就是非线性关系。然后就发现和一些非线性函数，满足一定得性质（这个得看人家的论文），通过非线性函数多层的组合，可以来拟合非常复杂的非线性函数。 更过关于激活函数的介绍，这里. TensorFlow中更多激活函数，Activation Function Binary step f(x) = \\begin{cases} 0, & \\text{for x $\\lt$ 0} \\\\ 1, & \\text{for x $\\geq$ 0} \\end{cases}阶跃函数是理想的激活函数，但是不可导。 下面对几种常见的激活函数进行可视化。 ReLu(rectified linear unit, ReLu) f(x) = \\begin{cases} 0, & \\text{for x $\\lt$ 0} \\\\ x, & \\text{for x $\\geq$ 0} \\end{cases}Logistic(Sigmoid or Soft step) f(x)=\\sigma{(x)} = \\frac{1}{1+e^{-x}}TanH f(x)= \\tanh{(x)}= \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}Softplus f(x)= \\ln{(1+e^x)}Softmax f_{i}(x)= \\frac{e^x_{i}}{\\sum_{j=1}^{J}e^{e^i}}1234567891011121314151617181920212223242526272829303132333435363738import tensorflow as tf # machine learning framworkimport numpy as np # scientific computationimport matplotlib.pyplot as plt # data visualizationwith tf.Graph().as_default(), tf.Session() as sess: x_data = np.linspace(-6, 6, 256) y_relu = tf.nn.relu(x_data) y_sigmoid = tf.nn.sigmoid(x_data) y_tanh = tf.nn.tanh(x_data) y_softplus = tf.nn.softplus(x_data) y_softmax = tf.nn.softmax(x_data) # softmax is a special kind of activation function, it is about probablity y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax = sess.run([y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax]) # plot fig = plt.figure(1, figsize=(8, 6)) plt.subplot(221) plt.plot(x_data, y_relu, c='red', label='relu') plt.ylim(-1, 6) plt.legend(loc='best') plt.subplot(222) plt.plot(x_data, y_sigmoid, c='green', label='sigmoid') plt.ylim(-0.3, 1.5) plt.legend(loc='best') plt.subplot(223) plt.plot(x_data, y_tanh, c='blue', label='tanh') plt.ylim(-1.2, 1.2) plt.legend(loc='best') plt.subplot(224) plt.plot(x_data, y_softplus, c='yellow', label='softplus') plt.ylim(-0.3, 6) plt.legend(loc='best') plt.show() 1234567891011121314151617181920212223242526272829303132with tf.Graph().as_default(), tf.Session() as sess: x_data = np.linspace(-6, 6, 256) y_relu = tf.nn.relu(x_data) y_sigmoid = tf.nn.sigmoid(x_data) y_tanh = tf.nn.tanh(x_data) y_softplus = tf.nn.softplus(x_data) y_softmax = tf.nn.softmax(x_data) # softmax is a special kind of activation function, it is about probablity y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax = sess.run([y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax]) # plot fig = plt.figure(1, figsize=(8, 6)) plt.plot(x_data, y_relu, c='red', label='relu') plt.plot(x_data, y_sigmoid, c='green', label='sigmoid') plt.plot(x_data, y_tanh, c='blue', label='tanh') plt.plot(x_data, y_softplus, c='yellow', label='softplus') plt.xlim(-4, 4) plt.ylim(-1.2, 4) plt.legend(loc='best') # gca get current axes ax = plt.gca() ax.spines['right'].set_color('none') ax.spines['top'].set_color('none') ax.xaxis.set_ticks_position('bottom') ax.yaxis.set_ticks_position('left') ax.spines['bottom'].set_position(('data', 0)) # outward, axes ax.spines['left'].set_position(('data', 0)) plt.show()","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"},{"name":"Activation","slug":"Activation","permalink":"https://www.blankspace.cn/tags/Activation/"}]},{"title":"Placeholder","slug":"placeholder","date":"2018-08-13T07:05:47.000Z","updated":"2018-08-13T07:09:46.215Z","comments":true,"path":"2018/08/13/placeholder/","link":"","permalink":"https://www.blankspace.cn/2018/08/13/placeholder/","excerpt":"","text":"解释A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data. 顾名思义，占位符，没有数据先占位。是一种变量。之后按照sess.run(***, feed_dict={input: **})形式传输数据。 1234567import tensorflow as tfwith tf.Graph().as_default(), tf.Session() as sess: x = tf.placeholder(tf.float32) y = x**3 feed_dict = &#123;x:[1, 2, 3, 4]&#125; print(sess.run(y, feed_dict=feed_dict)) [ 1. 8. 27. 64.] 总结placeholder 与 feed_dict={} 总是一起出现的。placeholder允许没有数据先占位创建计算图，运行时通过feed_dict传入数据。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"},{"name":"placeholder","slug":"placeholder","permalink":"https://www.blankspace.cn/tags/placeholder/"}]},{"title":"Tensor Control","slug":"Tensor-Control","date":"2018-08-13T02:06:14.000Z","updated":"2018-08-13T09:46:18.331Z","comments":true,"path":"2018/08/13/Tensor-Control/","link":"","permalink":"https://www.blankspace.cn/2018/08/13/Tensor-Control/","excerpt":"","text":"张量形状形状shape，用来描述张量的大小和数量。张量的形状表示为列表的形式，其中第i个元素表示维度i的大小，列表的长度标书张量的阶（维数）。 阶 形状 维数 示例 0 [] 0-D 0 维张量。标量。 1 [D0] 1-D 形状为 [6] 的 1 维张量。 2 [D0, D1] 2-D 形状为 [4, 3] 的 2 维张量。 3 [D0, D1, D2] 3-D 形状为 [1, 2, 3] 的 3 维张量。 n [D0, D1, … Dn-1] n 维 形状为 [D0, D1, … Dn-1] 的张量。 文档中介绍得更详细。 123456789101112131415# show the shape of tensorimport tensorflow as tfg = tf.Graph()with g.as_default(): scalar = tf.ones([]) # a scalar / 0-D tensor :1 vector = tf.ones([6]) # a vector with 6 elements: [1,1,1 ,1,1,1] matrix = tf.ones([2, 3]) # a matrix with 2 rows and 3 columns with tf.Session() as sess: # use tf.get_shape() print(\"Scalar shape: \",scalar.get_shape(), \" value: \", sess.run(scalar)) print(\"Vector shape: \",vector.get_shape(), \" value: \", sess.run(vector)) print(\"Matrix shape: \",matrix.get_shape(), \" value: \", sess.run(matrix)) D:\\Anaconda3\\Anaconda3_py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Scalar shape: () value: 1.0 Vector shape: (6,) value: [1. 1. 1. 1. 1. 1.] Matrix shape: (2, 3) value: [[1. 1. 1.] [1. 1. 1.]] 获取张量形状可以通过查看张量对象的shape属性来获取。 1vector.shape TensorShape([Dimension(6)]) 1matrix.shape[1] Dimension(3) 获取张量的数据类型查看张量对象的dtype属性。 1matrix.dtype tf.float32 改变张量数据类型tf.cast 1234c = tf.constant([1,9,8,3])print(c.dtype)f = tf.cast(c, dtype=tf.float32)print(f.dtype) &lt;dtype: &#39;int32&#39;&gt; &lt;dtype: &#39;float32&#39;&gt; 获取张量的阶tf.rank() 1tf.rank(scalar) &lt;tf.Tensor &#39;Rank:0&#39; shape=() dtype=int32&gt; 张量切片对于n阶张量，要访问其中某一元素，需要制定n个索引。 :是Python切片语法，也意味着不要变更该维度。可以帮助访问张量的子向量，子矩阵和子张量。 Broadcasting | 广播tensorflow支持广播，借鉴了Numpy中的做法，Numpy Broadcasting. 数学中，相同形状的张量才能进行元素级的运算，例如相加和等于。由于广播，使得不同形状的张量运算可以像对标量进行运算一样。 当张量被广播时，相当于对张量进行复制，实际上并不复制，广播专门为实现性能优化而设计。 举例，假设你和四个小伙伴，年龄分别为[18, 17, 20, 22, 21],每年年龄+1,模拟这个过程 12345678910111213141516# 向量加法with tf.Graph().as_default(): # method 1 ages = tf.constant([18, 17, 20, 22, 21]) one = tf.constant([1, 1, 1, 1, 1]) new_ages = tf.add(ages, one) with tf.Session() as sess: print(new_ages.eval()) # method 2 one_ = tf.constant(1) new_ages_ = tf.add(ages, one_) with tf.Session() as sess: print(new_ages_.eval()) [19 18 21 23 22] [19 18 21 23 22] 张量变形可以使用tf.reshape()来改变张量的形状。 1234567891011121314151617181920212223import numpy as np arr = np.arange(1, 13).reshape(3, 4)np.random.shuffle(arr)with tf.Graph().as_default(): x = tf.constant(arr, dtype=tf.int32) # create a 3x4 matrix/ 2-D tensor reshaped_4x3_x = tf.reshape(x, [4, 3]) reshaped_2x6_x = tf.reshape(x, [2, 6]) reshaped_3x2x2_x = tf.reshape(x, [3, 2, 2]) # reshape the rank with tf.Session() as sess: print(\"Original matrix (3x4):\") print(x.eval()) print(\"Reshaped matrix (4x3)\") print(reshaped_4x3_x.eval()) print(\"Reshaped matrix (2x6)\") print(reshaped_2x6_x.eval()) print(\"Reshaped matrix (3x2x2)\") print(reshaped_3x2x2_x.eval()) Original matrix (3x4): [[ 9 10 11 12] [ 1 2 3 4] [ 5 6 7 8]] Reshaped matrix (4x3) [[ 9 10 11] [12 1 2] [ 3 4 5] [ 6 7 8]] Reshaped matrix (2x6) [[ 9 10 11 12 1 2] [ 3 4 5 6 7 8]] Reshaped matrix (3x2x2) [[[ 9 10] [11 12]] [[ 1 2] [ 3 4]] [[ 5 6] [ 7 8]]] 变量、初始化和赋值TensorFlow变量初始化不是自动进行的，调用tf.global_variables_initializer()。不初始化就会报错。 1234567with tf.Graph().as_default(): v = tf.Variable([3]) with tf.Session() as sess: try: sess.run(v) except tf.errors.FailedPreconditionError as e: print(\"Caught excepted error: \", e) Caught excepted error: Attempting to use uninitialized value Variable [[Node: _retval_Variable_0_0 = _Retval[T=DT_INT32, index=0, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](Variable)]] 123456789with tf.Graph().as_default(): v = tf.Variable([3]) with tf.Session() as sess: init = tf.global_variables_initializer() sess.run(init) try: print(sess.run(v)) except tf.errors.FailedPreconditionError as e: print(\"Caught excepted error: \", e) [3] assign要变更变量的值，使用tf.assign()指令，仅仅创建assign指令也不能起作用。和初始化一样，也需要运行赋值指令才能变更变量值。 123456789with tf.Graph().as_default(): v = tf.Variable([3]) with tf.Session() as sess: init = tf.global_variables_initializer() sess.run(init) assignment = tf.assign(v, [9]) print(v.eval()) # the variable has not been changed yet. sess.run(assignment) print(v.eval()) # now the variable is updated [3] [9] 评估张量eval()。方法仅在默认 tf.Session 处于活动状态时才起作用。Tensor.eval()会返回一个和张量内容相同的Numpy数组。 仅仅只有占位符的情况下无法进行评估。 123456with tf.Graph().as_default(): p = tf.placeholder(tf.float32) t = p+1.0 with tf.Session() as sess: # t.eval() # this will fail, since the placeholder did not give a value print(t.eval(feed_dict=&#123;p:23.3&#125;) )# this will success, because a value is fed to the placeholder 24.3 举例：模拟投掷两个骰子10次素材来自这里. 模拟《活着》中富贵儿赌钱投骰子（6个面，点数从1到6）的过程，在模拟中生成一个 10x4 二维张量，其中： 列 1 和 2 均存储一个骰子的一次投掷值。 列 3 存储同一行中列 1 和 2 的值的总和。 列 4 表示开大开小，若列 3 点数大于7，开大（如用1表示）；小于等于7开小（如用0表示）。 例如，第一行中可能会包含以下值： 列 1 存储 4 列 2 存储 3 列 3 存储 7 列 4 存储 0 要完成此任务，可能需要浏览 TensorFlow 文档。 问题：如何随机并分配值给变量？（TensorFlow不支持动态计算图）如何赋值十次，循环？最后张量结果如何表示？ 123456789101112131415161718192021222324252627282930# import numpy as npimport tensorflow as tfg = tf.Graph()with g.as_default(): # 使用随机均匀分布 tf.random_uniform 来模拟投掷 n 次, 不需要循环。 dice1 = tf.Variable(tf.random_uniform([10, 1], minval=1, maxval=7, dtype=tf.int32)) dice2 = tf.Variable(tf.random_uniform([10, 1], minval=1, maxval=7, dtype=tf.int32)) dice_sum = tf.add(dice1, dice2) seven = tf.constant(7) # 关于TensorFlow条件控制 # https://www.tensorflow.org/versions/r1.8/api_guides/python/control_flow_ops#Control_Flow_Operations comp = tf.cast(tf.greater(dice_sum, seven), tf.int32) # 使用 tf.concat 连接向量，axis=1 水平方向连接 result = tf.concat(values=[dice1, dice2, dice_sum, comp], axis=1) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(result.eval()) [[ 4 4 8 1] [ 6 1 7 0] [ 5 6 11 1] [ 4 2 6 0] [ 4 1 5 0] [ 5 3 8 1] [ 6 2 8 1] [ 4 2 6 0] [ 4 6 10 1] [ 1 3 4 0]]","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"},{"name":"Tensor","slug":"Tensor","permalink":"https://www.blankspace.cn/tags/Tensor/"}]},{"title":"Tensor","slug":"Tensor","date":"2018-08-13T02:05:48.000Z","updated":"2018-08-13T07:07:08.853Z","comments":true,"path":"2018/08/13/Tensor/","link":"","permalink":"https://www.blankspace.cn/2018/08/13/Tensor/","excerpt":"TensorTensorFlow中的Tensor就是张量，张量是任意维的数组。常用的张量有： constant（标量/常量）：零维数组，零阶张量。例如，0或一个字符串常量。 vector（向量/矢量）：一维数组，一阶张量。例如，[0]或[0, 1, 1, 2, 3, 5]. matrix（矩阵）:二维数组，二阶张量。例如，[[4, 9, 2], [3, 5, 7], [8, 1, 6]] 在TensorFlow中，张量的创建、销毁和控制由指令完成。典型的TensorFlow代码大多数都是指令。 GraphTensorFlow中的图，也叫作计算图或数据流图（数据库原理和软件工程中的数据流图看似百无一用），是一种数据结构。 TensorFlow程序可以选择创建一张或者多张图。 图的节点是指令；图的边是张量。（张量就像是在图中的流动，所以称为Tensor Flow）. 张量流经图，在每个节点由一个指令控制。一个指令的输出张量通常会变成后续指令的输入张量。 TensorFlow实现延迟执行模型，系统仅会根据相关节点的需求在需要时计算节点。 常量和张量都是图中的一种指令，常量是始终会返回同一张量值的指令，变量是会返回分配给它的任何张量的指令。 图必须要在TensorFlow会话Session中运行，会话存储了被会话运行的图的状态. 会话可以将图发布到多台设备上（假如程序在某个分布式计算框架上运行）。官方提供的分布式TensorFlow文档. TensorFlow 提供了一个默认图。不过，我们建议您明确创建自己的 Graph，以便跟踪状态（例如，您可能希望在每个单元格中使用一个不同的 Graph）。 举例，“海伦-秦九韶公式”三斜求积 S = \\sqrt{p\\left( p-a\\right)\\left( p-b\\right)\\left( p-c\\right)} p=\\frac{a+b+c}{2}相关API.","text":"TensorTensorFlow中的Tensor就是张量，张量是任意维的数组。常用的张量有： constant（标量/常量）：零维数组，零阶张量。例如，0或一个字符串常量。 vector（向量/矢量）：一维数组，一阶张量。例如，[0]或[0, 1, 1, 2, 3, 5]. matrix（矩阵）:二维数组，二阶张量。例如，[[4, 9, 2], [3, 5, 7], [8, 1, 6]] 在TensorFlow中，张量的创建、销毁和控制由指令完成。典型的TensorFlow代码大多数都是指令。 GraphTensorFlow中的图，也叫作计算图或数据流图（数据库原理和软件工程中的数据流图看似百无一用），是一种数据结构。 TensorFlow程序可以选择创建一张或者多张图。 图的节点是指令；图的边是张量。（张量就像是在图中的流动，所以称为Tensor Flow）. 张量流经图，在每个节点由一个指令控制。一个指令的输出张量通常会变成后续指令的输入张量。 TensorFlow实现延迟执行模型，系统仅会根据相关节点的需求在需要时计算节点。 常量和张量都是图中的一种指令，常量是始终会返回同一张量值的指令，变量是会返回分配给它的任何张量的指令。 图必须要在TensorFlow会话Session中运行，会话存储了被会话运行的图的状态. 会话可以将图发布到多台设备上（假如程序在某个分布式计算框架上运行）。官方提供的分布式TensorFlow文档. TensorFlow 提供了一个默认图。不过，我们建议您明确创建自己的 Graph，以便跟踪状态（例如，您可能希望在每个单元格中使用一个不同的 Graph）。 举例，“海伦-秦九韶公式”三斜求积 S = \\sqrt{p\\left( p-a\\right)\\left( p-b\\right)\\left( p-c\\right)} p=\\frac{a+b+c}{2}相关API. 12345678910111213141516171819202122import tensorflow as tf# create a graphg = tf.Graph()# establish g as the \"default\" graphwith g.as_default(): a = tf.constant(3.0, name=\"a\") b = tf.constant(4.0, name=\"b\") c = tf.constant(5.0, name=\"c\") two = tf.constant(2.0) p = tf.divide(tf.add(a, tf.add(b, c)), two) pa = tf.subtract(p, a) pb = tf.subtract(p, b) pc = tf.subtract(p, c) s = tf.sqrt(tf.multiply(tf.multiply(p, pa), tf.multiply(pb, pc))) with tf.Session() as sess: print(s.eval()) 6.0 总结TensorFlow编程本质是两个步骤： 将常量、变量和指令整合到一个图中； 在一个会话中评估这些常量、变量和指令。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"},{"name":"Tensor","slug":"Tensor","permalink":"https://www.blankspace.cn/tags/Tensor/"}]},{"title":"Variable","slug":"Variable","date":"2018-08-12T14:35:05.000Z","updated":"2018-08-12T14:38:47.844Z","comments":true,"path":"2018/08/12/Variable/","link":"","permalink":"https://www.blankspace.cn/2018/08/12/Variable/","excerpt":"","text":"《庄子·天下》中说“一尺之棰，日取其半，万世不竭。”使用TensorFlow来模拟这个过程。 Variable1234567891011import tensorflow as tf# 定义变量bar = tf.Variable(1.0, name=\"bar\")two = tf.constant(2.0) divided = tf.divide(bar, 2.0) # 定义取半操作，这一步并没有直接计算# 将bar更新为新的长度update = tf.assign(bar, divided) 初始化变量global_variables_initializer() 1init = tf.global_variables_initializer() # z定义了变量就必须要初始化 创建会话并运行12345with tf.Session() as sess: sess.run(init) for i in range(10): sess.run(update) print('day %d len: '%i, sess.run(bar)) day 0 len: 0.5 day 1 len: 0.25 day 2 len: 0.125 day 3 len: 0.0625 day 4 len: 0.03125 day 5 len: 0.015625 day 6 len: 0.0078125 day 7 len: 0.00390625 day 8 len: 0.001953125 day 9 len: 0.0009765625 总结Variable就是变量，和变量相对的就是常量，常量就是不变的量，那么变量就是会变化的量。在TensorFlow中使用tf.Variable()来定义变量。定义了变量就必须要进行变量初始化，初始化使用tf.global_variables_initializer()tf.initialize_all_variables().变量在运行时也需依靠定义会话Session来run才行。 1234567891011121314151617# 高斯的故事之高斯求和TensorFlow实现s = tf.Variable(0, name=\"sum\")number = tf.Variable(0, name=\"number\")one = tf.constant(1)new_number = tf.assign(number, tf.add(number, one))update = tf.assign(s, tf.add(s, new_number))init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for i in range(1, 101): sess.run(update)print(sess.run(s))sess.close() 5050","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"}]},{"title":"Session","slug":"Session","date":"2018-08-12T14:28:06.000Z","updated":"2018-08-12T14:44:25.331Z","comments":true,"path":"2018/08/12/Session/","link":"","permalink":"https://www.blankspace.cn/2018/08/12/Session/","excerpt":"","text":"举两个例子。 Hello World123456789101112import tensorflow as tfnode = tf.constant(\"Hello, world!\")# method 1sess = tf.Session()print(sess.run(node))sess.close# method 2with tf.Session() as tf: print(sess.run(node)) D:\\Anaconda3\\Anaconda3_py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters b&#39;Hello, world!&#39; b&#39;Hello, world!&#39; 矩阵乘法理解Session会话，并使用TensorFlow来解决买菜问题（矩阵乘法）。素材来自《如何让11岁表妹知道矩阵乘法的本质是什么？》. 假设买肉和菜，农贸市场一斤肉25元，一斤菜3元。第一天我去农贸买了1斤肉，3斤菜，问计算用了多少钱：答：$25\\times1+3\\times3=34$。表示成向量乘法： \\begin{bmatrix} 25&3 \\\\ \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 3 \\\\ \\end{bmatrix} = 34第二天，又去农贸市场买了2斤肉，1斤菜,则两天的花销可以表示为： \\begin{bmatrix} 25&3 \\\\ \\end{bmatrix} \\times \\begin{bmatrix} 1&2 \\\\ 3&1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 34 & 53 \\end{bmatrix}左边矩阵形状为1x2，行就表示农贸市场，两列分别表示肉的单价和菜的单价.右边矩阵，第一列表示第一天，第二列表示第二天，依次类推。第一行表示买的肉的数量，第二行表示买的菜的数量。两者进行矩阵相乘，得到的就是农贸市场，第n天，买肉和菜的总和。 听说王大妈哪里肉一斤20，菜一斤2元，假如这两天在王大妈哪里买菜，表示成矩阵： \\begin{bmatrix} 25&3\\\\ 20&2\\\\ \\end{bmatrix} \\begin{bmatrix} 1&2\\\\ 3&1\\\\ \\end{bmatrix} = \\begin{bmatrix} 34&53\\\\ 26&42\\\\ \\end{bmatrix} 12345678910111213141516171819import tensorflow as tfimport numpy as np# 创建计算图mat1 = tf.constant(np.array([[25, 3], [20, 2]]))mat2 = tf.constant(np.array([[1, 2], [3, 1]]))product = tf.matmul(mat1, mat2)# 创建会话和运行# method 1sess = tf.Session()money = sess.run(product)print(money)sess.close()# method 2with tf.Session() as sess: money = sess.run(product) print(money) [[34 53] [26 42]] [[34 53] [26 42]]","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"}]},{"title":"MNIST pros","slug":"MNIST-pros","date":"2018-08-12T02:11:47.000Z","updated":"2018-08-12T02:16:34.195Z","comments":true,"path":"2018/08/12/MNIST-pros/","link":"","permalink":"https://www.blankspace.cn/2018/08/12/MNIST-pros/","excerpt":"运行TensorFlow的InteractiveSession/交互式会话sess = tf.InteractiveSession()用来创建交互式的会话，其余使用同Session 12import numpy as npimport tensorflow as tf D:\\Anaconda3\\Anaconda3_py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 准备工作和上一节类似 1234567# http://yann.lecun.com/exdb/mnist/ to download mnist dataset# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py# to read datasetfrom load_mnist import *import tensorflow.examples.tutorials.mnist.input_data as input_dataminst_dir = \"MNIST_data/\"mnist = input_data.read_data_sets(minst_dir, one_hot=True) 12x = tf.placeholder(\"float\", shape=[None, 784]) y_ = tf.placeholder(\"float\", shape=[None, 10]) # true label 构建一个多层卷积网络权重初始化12345678def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) # The generated values follow a normal distribution with specified mean and # standard deviation.从截断的正态分布中输出随机值，中如果x的取值在区间（μ-2σ，μ+2σ）之外则重新进行选择 return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) # Constant 2-D tensor populated with scalar value 0.1. 初始值为0.1，广播到shape形状的二维数组 return tf.Variable(initial) 卷积和汇聚（池化）由于Pooling翻译成池化让人不明所以，个人习惯依据其作用，而称之为汇聚。 简单起见，卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling. 12345def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')","text":"运行TensorFlow的InteractiveSession/交互式会话sess = tf.InteractiveSession()用来创建交互式的会话，其余使用同Session 12import numpy as npimport tensorflow as tf D:\\Anaconda3\\Anaconda3_py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 准备工作和上一节类似 1234567# http://yann.lecun.com/exdb/mnist/ to download mnist dataset# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py# to read datasetfrom load_mnist import *import tensorflow.examples.tutorials.mnist.input_data as input_dataminst_dir = \"MNIST_data/\"mnist = input_data.read_data_sets(minst_dir, one_hot=True) 12x = tf.placeholder(\"float\", shape=[None, 784]) y_ = tf.placeholder(\"float\", shape=[None, 10]) # true label 构建一个多层卷积网络权重初始化12345678def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) # The generated values follow a normal distribution with specified mean and # standard deviation.从截断的正态分布中输出随机值，中如果x的取值在区间（μ-2σ，μ+2σ）之外则重新进行选择 return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) # Constant 2-D tensor populated with scalar value 0.1. 初始值为0.1，广播到shape形状的二维数组 return tf.Variable(initial) 卷积和汇聚（池化）由于Pooling翻译成池化让人不明所以，个人习惯依据其作用，而称之为汇聚。 简单起见，卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling. 12345def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') 第一层卷积它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。 12345678910W_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32])# convert x to 4d vectorx_image = tf.reshape(x, [-1, 28,28, 1])# num x height x width x channel# We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool. # 我们把x_image和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pooling。h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1)+b_conv1)h_pool1 = max_pool_2x2(h_conv1) 第二层卷积为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。 12345W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2)+b_conv2)h_pool2 = max_pool_2x2(h_conv2) 密集连接（全连接）层经过两层卷积（激活）汇聚组合拳，原本的28x28的图像，特征图的尺寸减小到7x7（主要得益于汇聚）。并且一张图片有64个特征图（得益于卷积）。引入一个有1024个神经元的全连接层，来处理整个图片。 12345W_fc1 = weight_variable([7*7*64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1)+b_fc1) Dropout好像还没有官方中文翻译，那就叫它断电吧。使用TensorFlow构建计算图和使用vhdl构建电路有很多相似之处 减少过拟合。用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。 12keep_prob = tf.placeholder(\"float\")h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 输出层最后添加一个softmax层，如同前一节的softmax回归一样。 1234W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2)+b_fc2) # 输出层不需要激活函数 训练和评估我们会用更加复杂的ADAM优化器来做梯度最速下降，在feed_dict中加入额外的参数keep_prob来控制dropout比例。然后每100次迭代输出一次日志。 1import time 12345678910111213141516171819202122232425cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) # 优化器只是按照规则（参数更新算法）来更新参数的correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) # acc ratesess = tf.InteractiveSession()sess.run(tf.global_variables_initializer())print('Training...')start_t = time.clock()for step in range(20001): batch = mnist.train.next_batch(50) if step%100==0: train_accuracy = accuracy.eval(feed_dict=&#123;x:batch[0], y_:batch[1], keep_prob:1.0&#125;) print(\"Step %d, training accuracy %f\"%(step, train_accuracy)) train_step.run(feed_dict=&#123;x:batch[0], y_:batch[1], keep_prob:0.5&#125;)end_t = time.clock()print(\"Train Time Cost: \", end_t-start_t)print('Testing...')print(\"Test accuracy %f\"%accuracy.eval(feed_dict=&#123;x:mnist.test.images, y_:mnist.test.labels, keep_prob:1.0&#125;))sess.close() Training... Step 0, training accuracy 0.120000 Step 100, training accuracy 0.780000 Step 200, training accuracy 0.860000 Step 300, training accuracy 0.860000 Step 400, training accuracy 0.980000 Step 500, training accuracy 0.860000 Step 600, training accuracy 0.940000 Step 700, training accuracy 0.940000 Step 800, training accuracy 0.960000 Step 900, training accuracy 0.940000 Step 1000, training accuracy 0.960000 Step 1100, training accuracy 1.000000 Step 1200, training accuracy 0.960000 Step 1300, training accuracy 0.980000 Step 1400, training accuracy 1.000000 Step 1500, training accuracy 0.960000 Step 1600, training accuracy 0.940000 Step 1700, training accuracy 0.940000 Step 1800, training accuracy 0.940000 Step 1900, training accuracy 0.980000 Step 2000, training accuracy 0.980000 Step 2100, training accuracy 0.960000 Step 2200, training accuracy 0.940000 Step 2300, training accuracy 0.980000 Step 2400, training accuracy 0.980000 Step 2500, training accuracy 0.960000 Step 2600, training accuracy 1.000000 Step 2700, training accuracy 1.000000 Step 2800, training accuracy 0.980000 Step 2900, training accuracy 1.000000 Step 3000, training accuracy 0.980000 Step 3100, training accuracy 1.000000 Step 3200, training accuracy 0.980000 Step 3300, training accuracy 1.000000 Step 3400, training accuracy 1.000000 Step 3500, training accuracy 0.960000 Step 3600, training accuracy 1.000000 Step 3700, training accuracy 0.980000 Step 3800, training accuracy 1.000000 Step 3900, training accuracy 0.980000 Step 4000, training accuracy 0.980000 Step 4100, training accuracy 1.000000 Step 4200, training accuracy 0.980000 Step 4300, training accuracy 0.960000 Step 4400, training accuracy 1.000000 Step 4500, training accuracy 1.000000 Step 4600, training accuracy 1.000000 Step 4700, training accuracy 1.000000 Step 4800, training accuracy 1.000000 Step 4900, training accuracy 0.980000 Step 5000, training accuracy 0.980000 Step 5100, training accuracy 0.980000 Step 5200, training accuracy 1.000000 Step 5300, training accuracy 1.000000 Step 5400, training accuracy 1.000000 Step 5500, training accuracy 1.000000 Step 5600, training accuracy 1.000000 Step 5700, training accuracy 0.980000 Step 5800, training accuracy 1.000000 Step 5900, training accuracy 0.980000 Step 6000, training accuracy 0.980000 Step 6100, training accuracy 0.980000 Step 6200, training accuracy 1.000000 Step 6300, training accuracy 0.980000 Step 6400, training accuracy 0.980000 Step 6500, training accuracy 1.000000 Step 6600, training accuracy 0.980000 Step 6700, training accuracy 1.000000 Step 6800, training accuracy 1.000000 Step 6900, training accuracy 1.000000 Step 7000, training accuracy 1.000000 Step 7100, training accuracy 0.980000 Step 7200, training accuracy 1.000000 Step 7300, training accuracy 1.000000 Step 7400, training accuracy 0.980000 Step 7500, training accuracy 1.000000 Step 7600, training accuracy 0.980000 Step 7700, training accuracy 0.980000 Step 7800, training accuracy 1.000000 Step 7900, training accuracy 0.980000 Step 8000, training accuracy 1.000000 Step 8100, training accuracy 1.000000 Step 8200, training accuracy 0.980000 Step 8300, training accuracy 0.980000 Step 8400, training accuracy 1.000000 Step 8500, training accuracy 0.980000 Step 8600, training accuracy 1.000000 Step 8700, training accuracy 0.980000 Step 8800, training accuracy 0.980000 Step 8900, training accuracy 1.000000 Step 9000, training accuracy 1.000000 Step 9100, training accuracy 1.000000 Step 9200, training accuracy 1.000000 Step 9300, training accuracy 0.980000 Step 9400, training accuracy 1.000000 Step 9500, training accuracy 0.960000 Step 9600, training accuracy 0.980000 Step 9700, training accuracy 1.000000 Step 9800, training accuracy 1.000000 Step 9900, training accuracy 1.000000 Step 10000, training accuracy 1.000000 Step 10100, training accuracy 1.000000 Step 10200, training accuracy 1.000000 Step 10300, training accuracy 1.000000 Step 10400, training accuracy 1.000000 Step 10500, training accuracy 1.000000 Step 10600, training accuracy 1.000000 Step 10700, training accuracy 1.000000 Step 10800, training accuracy 0.980000 Step 10900, training accuracy 1.000000 Step 11000, training accuracy 1.000000 Step 11100, training accuracy 1.000000 Step 11200, training accuracy 0.980000 Step 11300, training accuracy 1.000000 Step 11400, training accuracy 1.000000 Step 11500, training accuracy 0.980000 Step 11600, training accuracy 0.980000 Step 11700, training accuracy 0.960000 Step 11800, training accuracy 1.000000 Step 11900, training accuracy 1.000000 Step 12000, training accuracy 0.980000 Step 12100, training accuracy 1.000000 Step 12200, training accuracy 0.980000 Step 12300, training accuracy 1.000000 Step 12400, training accuracy 1.000000 Step 12500, training accuracy 1.000000 Step 12600, training accuracy 1.000000 Step 12700, training accuracy 1.000000 Step 12800, training accuracy 1.000000 Step 12900, training accuracy 1.000000 Step 13000, training accuracy 1.000000 Step 13100, training accuracy 1.000000 Step 13200, training accuracy 1.000000 Step 13300, training accuracy 1.000000 Step 13400, training accuracy 1.000000 Step 13500, training accuracy 1.000000 Step 13600, training accuracy 1.000000 Step 13700, training accuracy 1.000000 Step 13800, training accuracy 1.000000 Step 13900, training accuracy 1.000000 Step 14000, training accuracy 1.000000 Step 14100, training accuracy 1.000000 Step 14200, training accuracy 1.000000 Step 14300, training accuracy 1.000000 Step 14400, training accuracy 1.000000 Step 14500, training accuracy 1.000000 Step 14600, training accuracy 0.980000 Step 14700, training accuracy 1.000000 Step 14800, training accuracy 1.000000 Step 14900, training accuracy 1.000000 Step 15000, training accuracy 0.980000 Step 15100, training accuracy 1.000000 Step 15200, training accuracy 1.000000 Step 15300, training accuracy 1.000000 Step 15400, training accuracy 1.000000 Step 15500, training accuracy 1.000000 Step 15600, training accuracy 1.000000 Step 15700, training accuracy 1.000000 Step 15800, training accuracy 1.000000 Step 15900, training accuracy 1.000000 Step 16000, training accuracy 1.000000 Step 16100, training accuracy 1.000000 Step 16200, training accuracy 1.000000 Step 16300, training accuracy 1.000000 Step 16400, training accuracy 1.000000 Step 16500, training accuracy 1.000000 Step 16600, training accuracy 1.000000 Step 16700, training accuracy 0.960000 Step 16800, training accuracy 1.000000 Step 16900, training accuracy 1.000000 Step 17000, training accuracy 1.000000 Step 17100, training accuracy 1.000000 Step 17200, training accuracy 0.980000 Step 17300, training accuracy 1.000000 Step 17400, training accuracy 1.000000 Step 17500, training accuracy 1.000000 Step 17600, training accuracy 1.000000 Step 17700, training accuracy 1.000000 Step 17800, training accuracy 1.000000 Step 17900, training accuracy 1.000000 Step 18000, training accuracy 1.000000 Step 18100, training accuracy 1.000000 Step 18200, training accuracy 1.000000 Step 18300, training accuracy 1.000000 Step 18400, training accuracy 1.000000 Step 18500, training accuracy 1.000000 Step 18600, training accuracy 1.000000 Step 18700, training accuracy 1.000000 Step 18800, training accuracy 1.000000 Step 18900, training accuracy 1.000000 Step 19000, training accuracy 1.000000 Step 19100, training accuracy 1.000000 Step 19200, training accuracy 1.000000 Step 19300, training accuracy 1.000000 Step 19400, training accuracy 1.000000 Step 19500, training accuracy 1.000000 Step 19600, training accuracy 1.000000 Step 19700, training accuracy 1.000000 Step 19800, training accuracy 1.000000 Step 19900, training accuracy 0.980000 Step 20000, training accuracy 1.000000 Train Time Cost: 205.04848637568546 Testing... Test accuracy 0.992300 总结本节对mnist数据集，使用TensorFlow构建了卷积池化API，并构建了两层卷积池化层，一层全连接层的神经网络，使用dropout来减少过拟合，最后输出到softmax回归层，从而构建了完整的卷积神经网络模型。优化器使用Adam，经过2万次迭代的训练，在测试集上精确度达到了(99%).","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"},{"name":"mnist","slug":"mnist","permalink":"https://www.blankspace.cn/tags/mnist/"}]},{"title":"TensorFlow tutorial mnist beginners","slug":"TensorFlow-tutorial-mnist-beginners","date":"2018-08-11T09:25:06.000Z","updated":"2018-08-12T02:12:56.707Z","comments":true,"path":"2018/08/11/TensorFlow-tutorial-mnist-beginners/","link":"","permalink":"https://www.blankspace.cn/2018/08/11/TensorFlow-tutorial-mnist-beginners/","excerpt":"","text":"从示例开始导入12import tensorflow as tfimport numpy as np # 和tf配合使用 D:\\Anaconda3\\Anaconda3_py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters 构造线性模型12345678910111213141516# generate phony data | 生成假数据# 实际应用应该是面对真实的数据# 100个点，（2x100）x_data = np.float32(np.random.rand(2, 100))y_data = np.dot([0.100, 0.200], x_data)+0.300# 构造线性模型b = tf.Variable(tf.zeros([1])) # tensor of shape 1W = tf.Variable(tf.random_uniform([1, 2], minval=-0.1, maxval=1.0)) # tensor of shape 1x2 whose values are from a uniform distribution # in the range of [minval, maxval)y = tf.matmul(W, x_data)+b# 最小化方差loss = tf.reduce_mean(tf.square(y-y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss) 123# 初始化变量# init = tf.initialize_all_variables()init = tf.global_variables_initializer() 123# 启动计算图(graph)sess = tf.Session() # 创建会话sess.run(init) 12345# 拟合平面for step in range(0, 201): sess.run(train) if step%20==0: print(\"Step: \", step, \" W= \",sess.run(W), \" b= \", sess.run(b)) Step: 0 W= [[0.4871506 0.41139394]] b= [-0.09778017] Step: 20 W= [[0.22539519 0.34252512]] b= [0.15573283] Step: 40 W= [[0.14763923 0.26675704]] b= [0.23892412] Step: 60 W= [[0.11975352 0.22945689]] b= [0.2737922] Step: 80 W= [[0.10842387 0.21279107]] b= [0.2887098] Step: 100 W= [[0.10362242 0.20552918]] b= [0.2951307] Step: 120 W= [[0.10156146 0.20238699]] b= [0.29789925] Step: 140 W= [[0.10067356 0.2010301 ]] b= [0.2990936] Step: 160 W= [[0.10029061 0.20044449]] b= [0.29960892] Step: 180 W= [[0.10012538 0.20019177]] b= [0.29983127] Step: 200 W= [[0.10005409 0.20008272]] b= [0.2999272] 12# 关闭会话sess.close() MNIST机器学习入门1234# http://yann.lecun.com/exdb/mnist/ to download mnist dataset# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py# to read datasetfrom load_mnist import * 123import tensorflow.examples.tutorials.mnist.input_data as input_dataminst_dir = \"MNIST_data/\"mnist = input_data.read_data_sets(minst_dir, one_hot=True) Extracting MNIST_data/train-images-idx3-ubyte.gz Extracting MNIST_data/train-labels-idx1-ubyte.gz Extracting MNIST_data/t10k-images-idx3-ubyte.gz Extracting MNIST_data/t10k-labels-idx1-ubyte.gz WARNING:tensorflow:From D:\\Anaconda3\\Anaconda3_py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. 下载下来的数据集被分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）. 每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。我们把这些图片设为“xs”，把这些标签设为“ys”。训练数据集和测试数据集都包含xs和ys，比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels. 每一张图片包含28像素X28像素。我们可以用一个数字数组来表示这张图片： 我们把这个数组展开成一个向量，长度是 28x28 = 784。如何展开这个数组（数字间的顺序）不重要，只要保持各个图片采用相同的方式展开。从这个角度来看，MNIST数据集的图片就是在784维向量空间里面的点, 并且拥有比较复杂的结构 (提醒: 此类数据的可视化是计算密集型的)。 在MNIST训练数据集中，mnist.train.images 是一个形状为 [60000, 784]的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。 相对应的MNIST数据集的标签是介于0到9的数字，用来描述给定图片里表示的数字。为了用于这个教程，我们使标签数据是”one-hot vectors”。 一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0。所以在此教程中，数字n将表示成一个只有在第n维度（从0开始）数字为1的10维向量。比如，标签0将表示成([1,0,0,0,0,0,0,0,0,0,0])。因此， mnist.train.labels 是一个[60000, 10]的数字矩阵。 1234567# x represent the input# None means number of images is adaptable# a 28x28 image is flatten to a vector of 784x = tf.placeholder(\"float\", [None, 784])W = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10])) Softmax regression关于Softmax可以阅读这篇或者这篇. 整个回归模型： 12# softmaxy = tf.nn.softmax(tf.matmul(x, W)+b) 训练模型使用交叉熵损失函数，更多介绍看这篇. 1234y_ = tf.placeholder(\"float\", [None, 10])# 注意，这里的交叉熵不仅仅用来衡量单一的一对预测和真实值，而是所有100幅图片的交叉熵的总和cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 12# gradient descent and backprapagationtrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) 更多优化算法点击这里. 12# init graphinit = tf.global_variables_initializer() 123456with tf.Session() as sess: sess.run(init) for step in range(1001): batch_xs, batch_ys = mnist.train.next_batch(100) feed_dict =&#123;x: batch_xs, y_:batch_ys&#125; sess.run(train_step, feed_dict) 评估我们的模型tf.argmax 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值，而 tf.argmax(y_,1) 代表正确的标签，我们可以用 tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。 12345678910with tf.Session() as sess: sess.run(init) for step in range(1001): batch_xs, batch_ys = mnist.train.next_batch(100) feed_dict =&#123;x: batch_xs, y_:batch_ys&#125; sess.run(train_step, feed_dict) if step%100==0: correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print(step, \" train acc: \", sess.run(accuracy, feed_dict)) 0 train acc: 0.48 100 train acc: 0.94 200 train acc: 0.96 300 train acc: 0.93 400 train acc: 0.93 500 train acc: 0.89 600 train acc: 0.96 700 train acc: 0.94 800 train acc: 0.94 900 train acc: 0.95 1000 train acc: 0.98 1234567891011121314with tf.Session() as sess: sess.run(init) for step in range(1001): batch_xs, batch_ys = mnist.train.next_batch(100) feed_dict =&#123;x: batch_xs, y_:batch_ys&#125; sess.run(train_step, feed_dict)# if step%100==0:# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))# accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))# print(step, \" train acc: \", sess.run(accuracy, feed_dict))# # for test set correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print(\"Test acc:\", sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y_:mnist.test.labels&#125;)) Test acc: 0.9138 更多内容https://www.tensorflow.org/tutorials/ 总结本文首先通过一个完整的线性回归的例子，展示了TensorFlow的基本使用，首先构造计算图graph，其中包含诸如Variable、placeholder等节点，以及节点之间的数学运算如矩阵乘法；接着对计算图初始化，tf.global_variables_initializer()，其中TensorFlow中很重要的就是回话session机制，通过会话来运行计算图；随后，通过会话进行训练，拟合并德大检测结果。 其次，使用TensorFlow完成了基于MNIST数据集，进行手写体识别的任务，并在测试集上实现了91%的正确率。其完整过程，首先是准备数据，下载的数据被分为训练数据和测试数据。每种数据的基本单元都包含两部分，一部分是手写体图片，统一规格28x28，另一部分是对应的标记，来指出对应的数字，将原始图片展开向量表示，标记采用one-hot表示；接着，构建softmax回归模型。使用梯度下降算法进行反向传播，最小化交叉熵损失进行训练。最后，在测试机上测试，得到实验正确率指标，可以达到91%以上。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://www.blankspace.cn/categories/Machine-Learning/"},{"name":"TensorFlow","slug":"Machine-Learning/TensorFlow","permalink":"https://www.blankspace.cn/categories/Machine-Learning/TensorFlow/"}],"tags":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://www.blankspace.cn/tags/TensorFlow/"},{"name":"mnist","slug":"mnist","permalink":"https://www.blankspace.cn/tags/mnist/"}]},{"title":"Code Highlight","slug":"code-hightlight","date":"2018-08-11T04:30:03.000Z","updated":"2018-08-11T06:04:04.219Z","comments":true,"path":"2018/08/11/code-hightlight/","link":"","permalink":"https://www.blankspace.cn/2018/08/11/code-hightlight/","excerpt":"","text":"123456789101112131415161718192021/* * This is some sample code to illustrate how things look! */import Musician from './liverpool';class Paul extends Musician &#123; constructor(bass) &#123; super(bass); &#125; get fullName() &#123; return 'Paul McCartney'; &#125; perform() &#123; this.play(this.instrument); this.sing(); &#125;&#125;export default Paul;","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"},{"name":"Code","slug":"Code","permalink":"https://www.blankspace.cn/tags/Code/"}]},{"title":"Numpy随机数1","slug":"Numpy随机数","date":"2018-08-11T02:45:00.000Z","updated":"2018-08-11T11:37:44.854Z","comments":true,"path":"2018/08/11/Numpy随机数/","link":"","permalink":"https://www.blankspace.cn/2018/08/11/Numpy随机数/","excerpt":"","text":"更多内容查看这里 Simple random data | 常用随机函数1import numpy as np 123# rand(d0, d1, …, dn)# random values in a given shapenp.random.rand(1) array([0.15771123]) 1np.random.rand(3, 4) array([[0.17303082, 0.13546342, 0.96790769, 0.54289967], [0.47752797, 0.3163813 , 0.33804729, 0.63760302], 更多内容查看这里 Simple random data | 常用随机函数1import numpy as np rand(d0, d1, …, dn)123# rand(d0, d1, …, dn)# random values in a given shapenp.random.rand(1) array([0.97299322]) 1np.random.rand(3, 4) array([[0.81969546, 0.63076449, 0.70050141, 0.28701361], [0.5909755 , 0.10265285, 0.03841936, 0.04747027], [0.50586629, 0.33692611, 0.18006999, 0.37508064]]) 1np.random.rand(5) array([0.4027442 , 0.29275407, 0.15175853, 0.76296697, 0.84970737]) 正态分布随机数 randn(d0, d1, …, dn)123# randn(d0, d1, …, dn)# return a sample or samples from standard norma sidtributionnp.random.randn(1) array([0.68010349]) 1np.random.randn(4) array([-0.22561174, -0.12611506, 1.42778485, 0.69267162]) 1np.random.randn(2, 2) array([[ 1.09486998, -0.12217683], [-0.26184258, 0.70775913]]) Note: We can use matplotlib to show the distrubution 随机整数randint(low[, high, size, dtype]) / np.random.random_integers(2) 123# randint(low[, high, size, dtype])# return random integers from low(inclusive) to high(exclusive)np.random.randint(1) 0 1np.random.randint(3) 2 1np.random.randint(100, 1000) 945 1np.random.randint(0, 100, 5) array([76, 86, 29, 40, 76]) 1np.random.randint(0, 100, (3, 3)) array([[88, 50, 90], [64, 69, 57], [ 2, 10, 95]]) 12345# random_integers(low[, high, size])# random integers of type np.int between low and high, inclusivenp.random.random_integers(2)# This function is deprecated. Please call randint(l, r + 1) instead D:\\Anaconda3\\Anaconda3_py36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: This function is deprecated. Please call randint(1, 2 + 1) instead This is separate from the ipykernel package so we can avoid doing imports until 1 随机小数 random_sample([size]) random([size]) ranf([size]) sample([size]) 123# random_sample([size])# return random floats in the half-open interval[0.0, 1.0)np.random.random_sample(1) array([0.42922028]) 1np.random.random_sample((3,3)) array([[0.13911191, 0.29153576, 0.38330187], [0.39095818, 0.37114282, 0.69224038], [0.3664121 , 0.02533538, 0.76235701]]) 123# random([size])# return random floats in the half-open interval[0.0, 1.0)np.random.random(1) array([0.57758511]) 1np.random.random((2, 2)) array([[0.89365816, 0.10505705], [0.76704421, 0.27889437]]) 123# ranf([size])# return random floats int the half-open interval [0.0, 1.0)np.random.ranf(1) array([0.64518204]) 1np.random.ranf((3,4)) array([[0.10753966, 0.70902024, 0.63834775, 0.65623879], [0.71709776, 0.34437971, 0.42639926, 0.70715686], [0.12977266, 0.20338805, 0.94316098, 0.3082274 ]]) 123# sample([size])# return random floats int the half-open interval [0.0, 1.0)np.random.sample(2) array([0.95896772, 0.64870791]) 1np.random.sample((2, 2)) array([[0.91634732, 0.95899658], [0.59189998, 0.53045256]]) 自定义随机 choice(a[, size, replace, p]) 123# choice(a[, size, replace, p])# generate a random sample from a given 1-D arraynp.random.choice([3, 2, 1, 0],3) array([3, 1, 3]) 1np.random.choice([3, 2, 1, 0],3, replace=False) array([1, 2, 3]) 1np.random.choice([\"cat\", \"dog\", \"fish\", \"mouse\", \"bird\"],(2, 2), replace=True) array([[&#39;cat&#39;, &#39;dog&#39;], [&#39;bird&#39;, &#39;dog&#39;]], dtype=&#39;&lt;U5&#39;) 1234# p represents properties of the elements of the 1-D array# sum of p must be 1.0np.random.choice([\"cat\", \"dog\", \"fish\", \"mouse\", \"bird\"],(2, 2), replace=True, p=[0.1, 0.5, 0.3, 0.05, 0.05]) array([[&#39;fish&#39;, &#39;dog&#39;], [&#39;dog&#39;, &#39;fish&#39;]], dtype=&#39;&lt;U5&#39;) 123# bytes(length)# return random bytesnp.random.bytes(1) b&#39;\\x8c&#39; Permutations | 排列 shuffle(x) permutation(x) 123456789# shuffle(x)# modify a sequence in-place by shuffling its contents# 将序列在原地打乱x = np.arange(11)print('----befor shuffle x----')print(x)print('----after shuffle x----')np.random.shuffle(x)print(x) ----befor shuffle x---- [ 0 1 2 3 4 5 6 7 8 9 10] ----after shuffle x---- [ 8 1 5 4 0 2 3 9 7 6 10] 1234567# permutation(x)# randomly permute a sequence, or return a permuted rangex = np.arange(11)print(\"---permutation----\")print(np.random.permutation(x))print(\"---x----\")print(x) ---permutation---- [ 9 4 6 0 5 10 8 3 7 2 1] ---x---- [ 0 1 2 3 4 5 6 7 8 9 10] Random generator | 随机数生成器如果设置了seed，每次重新运行时生成的随机数都是确定和上次相同的。 1234# RandomState([seed])# container for the Mersenne Twister pseudo-random number generatornp.random.RandomState(1)np.random.random(5) array([0.58048047, 0.57948883, 0.82388234, 0.77501925, 0.70183176]) 1234# seed([seed])# seed the generatornp.random.seed(0)np.random.random(5) array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ]) 123456# get_state()# return a tuple representing the internal state of the generator # np.random.get_state()# set_state(state)# set the interna state of the generator from a tuple","categories":[{"name":"Languages","slug":"Languages","permalink":"https://www.blankspace.cn/categories/Languages/"},{"name":"Python","slug":"Languages/Python","permalink":"https://www.blankspace.cn/categories/Languages/Python/"},{"name":"Numpy","slug":"Languages/Python/Numpy","permalink":"https://www.blankspace.cn/categories/Languages/Python/Numpy/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"https://www.blankspace.cn/tags/Numpy/"},{"name":"random","slug":"random","permalink":"https://www.blankspace.cn/tags/random/"}]},{"title":"Emoji","slug":"emoji-text","date":"2018-08-10T12:20:02.000Z","updated":"2018-08-11T06:04:25.620Z","comments":true,"path":"2018/08/10/emoji-text/","link":"","permalink":"https://www.blankspace.cn/2018/08/10/emoji-text/","excerpt":"","text":"Input emoji with syntax :smile:. Sad, it is failed. 😂😁 Use inputs(eg. MS default input) which can input emoji directly, this is the easiest way. Here to know more.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"},{"name":"Emoji","slug":"Emoji","permalink":"https://www.blankspace.cn/tags/Emoji/"}]},{"title":"Mathjax","slug":"mathjax-test","date":"2018-08-10T11:53:22.000Z","updated":"2018-08-10T12:41:46.743Z","comments":true,"path":"2018/08/10/mathjax-test/","link":"","permalink":"https://www.blankspace.cn/2018/08/10/mathjax-test/","excerpt":"","text":"This post is used for testing mathjax(latex) support. Inline mathUse $...$, example: 1$\\lim_&#123;x \\to \\infty&#125; \\exp(-x) = 0$ Writing LaTex like: \\lim_{x \\to \\infty} \\exp(-x) = 0 Displayed formulasUse $$...$$ , example: 1$$\\sum_&#123;i=0&#125;^n i^2 = \\frac&#123;(n^2+n)(2n+1)&#125;&#123;6&#125;$$ \\sum_{i=0}^n i^2 = \\frac{(n^2+n)(2n+1)}{6}1234567$$ \\begin&#123;matrix&#125; 1 &amp; x &amp; x^2 \\\\ 1 &amp; y &amp; y^2 \\\\ 1 &amp; z &amp; z^2 \\\\ \\end&#123;matrix&#125;$$ \\begin{matrix} 1 & x & x^2 \\\\ 1 & y & y^2 \\\\ 1 & z & z^2 \\\\ \\end{matrix}1234567\\begin&#123;align&#125;\\sqrt&#123;37&#125; &amp; = \\sqrt&#123;\\frac&#123;73^2-1&#125;&#123;12^2&#125;&#125; \\\\ &amp; = \\sqrt&#123;\\frac&#123;73^2&#125;&#123;12^2&#125;\\cdot\\frac&#123;73^2-1&#125;&#123;73^2&#125;&#125; \\\\ &amp; = \\sqrt&#123;\\frac&#123;73^2&#125;&#123;12^2&#125;&#125;\\sqrt&#123;\\frac&#123;73^2-1&#125;&#123;73^2&#125;&#125; \\\\ &amp; = \\frac&#123;73&#125;&#123;12&#125;\\sqrt&#123;1 - \\frac&#123;1&#125;&#123;73^2&#125;&#125; \\\\ &amp; \\approx \\frac&#123;73&#125;&#123;12&#125;\\left(1 - \\frac&#123;1&#125;&#123;2\\cdot73^2&#125;\\right)\\end&#123;align&#125; \\begin{align} \\sqrt{37} & = \\sqrt{\\frac{73^2-1}{12^2}} \\\\ & = \\sqrt{\\frac{73^2}{12^2}\\cdot\\frac{73^2-1}{73^2}} \\\\ & = \\sqrt{\\frac{73^2}{12^2}}\\sqrt{\\frac{73^2-1}{73^2}} \\\\ & = \\frac{73}{12}\\sqrt{1 - \\frac{1}{73^2}} \\\\ & \\approx \\frac{73}{12}\\left(1 - \\frac{1}{2\\cdot73^2}\\right) \\end{align}1234567\\begin&#123;array&#125;&#123;c|lcr&#125;n &amp; \\text&#123;Left&#125; &amp; \\text&#123;Center&#125; &amp; \\text&#123;Right&#125; \\\\\\hline1 &amp; 0.24 &amp; 1 &amp; 125 \\\\2 &amp; -1 &amp; 189 &amp; -8 \\\\3 &amp; -20 &amp; 2000 &amp; 1+10i\\end&#123;array&#125; \\begin{array}{c|lcr} n & \\text{Left} & \\text{Center} & \\text{Right} \\\\ \\hline 1 & 0.24 & 1 & 125 \\\\ 2 & -1 & 189 & -8 \\\\ 3 & -20 & 2000 & 1+10i \\end{array}More information can be found Here. To know how to let hexo support Matjax, you can read this.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"}]},{"title":"Translation","slug":"translate","date":"2018-08-08T10:27:25.000Z","updated":"2018-08-13T10:38:36.792Z","comments":true,"path":"2018/08/08/translate/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/translate/","excerpt":"This post is for testing google translation widget.","text":"This post is for testing google translation widget. 采薇 采薇采薇、薇亦作止。 曰歸曰歸、歲亦莫止。 靡室靡家、玁狁之故。 不遑啟居、玁狁之故。 采薇采薇、薇亦柔止。 曰歸曰歸、心亦憂止。 憂心烈烈、載飢載渴。 我戌未定、靡使歸聘。 采薇采薇、薇亦剛止。 曰歸曰歸、歲亦陽止。 王事靡盬、不遑啟處。 憂心孔疚、我行不來。 駕彼四牡、四牡騤騤。 君子所依、小人所腓。 四牡翼翼、象弭魚服。 豈不日戒、玁狁孔亟。 彼爾維何、維常之華。 彼路斯何、君子之車。 戎車既駕、四牡業業。 豈敢定居、一月三捷。 駕彼四牡、四牡騤騤。 君子所依、小人所腓。 四牡翼翼、象弭魚服。 豈不日戒、玁狁孔亟。 昔我往矣、楊柳依依。 今我來思、雨雪靡靡。 行道遲遲、載渴載飢。 我心傷悲、莫知我哀。 CAI WEI Let us gather the thorn-ferns , let us gather the thorn-ferns ; The thorn-ferns are now springing up . When shall we return ? When shall we return ? It will be late in the [next] year . Wife and husband will be separated , Because of the Xian-yun . We shall have no leisure to rest , Because of the Xian-yun . Let us gather the thorn-ferns , let us gather the thorn-ferns ; The thorn-ferns are now tender . When shall we return ? When shall we return ? Our hearts are sorrowful ; Our hearts are sad and sorrowful ; We shall hunger , we shall thirst . While our service on guard is not finished , We can send no one home to enquire about our families . Let us gather the thorn-ferns , let us gather the thorn-ferns ; The thorn-ferns are now hard . When shall we return ? When shall we return ? The year will be in the tenth month . But the king’s business must not be slackly performed ; We shall have no leisure to rest . Our sorrowing hearts are in great distress ; But we shall not return from our expedition . What is that so gorgeous ? It is the flowers of the cherry tree . What carriage is that ? It is the carriage of our general . His war carriage is yoked ; The four steeds are strong . Dare we remain inactive ? In one month we shall have three victories . The four steeds are yoked , The four steeds , eager and strong ; The confidence of the general , The protection of the men . The four steeds move regularly , like wings ; here are the bow with its ivory ends , and the seal-skin quiver . Shall we not daily warn one another ? The business of the Xian-yun is very urgent . At first , when we set out , The willows were fresh and green ; Now , when we shall be returning , The snow will be falling in clouds . Long and tedious will be our marching ; We shall hunger ; we shall thirst . Our hearts are wounded with grief , And no one knows our sadness . 翻译来源：这里.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"}]},{"title":"Categories","slug":"categories","date":"2018-08-07T16:01:04.000Z","updated":"2018-08-10T02:53:31.192Z","comments":true,"path":"2018/08/08/categories/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/categories/","excerpt":"","text":"This post contains 3 categories. Make sure the theme can display all of the categories.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[]},{"title":"Elements","slug":"elements","date":"2018-08-07T16:01:03.000Z","updated":"2018-08-10T02:53:18.753Z","comments":true,"path":"2018/08/08/elements/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/elements/","excerpt":"","text":"The purpose of this post is to make sure all HTML elements can display properly. Heading 1Heading 2Heading 3Heading 4Heading 5Heading 6 ParagraphLorem ipsum dolor sit amet, test link consectetur adipiscing elit. Strong text pellentesque ligula commodo viverra vehicula. Italic text at ullamcorper enim. Morbi a euismod nibh. Underline text non elit nisl. Deleted text tristique, sem id condimentum tempus, metus lectus venenatis mauris, sit amet semper lorem felis a eros. Fusce egestas nibh at sagittis auctor. Sed ultricies ac arcu quis molestie. Donec dapibus nunc in nibh egestas, vitae volutpat sem iaculis. Curabitur sem tellus, elementum nec quam id, fermentum laoreet mi. Ut mollis ullamcorper turpis, vitae facilisis velit ultricies sit amet. Etiam laoreet dui odio, id tempus justo tincidunt id. Phasellus scelerisque nunc sed nunc ultricies accumsan. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed erat diam, blandit eget felis aliquam, rhoncus varius urna. Donec tellus sapien, sodales eget ante vitae, feugiat ullamcorper urna. Praesent auctor dui vitae dapibus eleifend. Proin viverra mollis neque, ut ullamcorper elit posuere eget. Praesent diam elit, interdum ut pulvinar placerat, imperdiet at magna. Maecenas ornare arcu at mi suscipit, non molestie tortor ultrices. Aenean convallis, diam et congue ultricies, erat magna tincidunt orci, pulvinar posuere mi sapien ac magna. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Praesent vitae placerat mauris. Nullam laoreet ante posuere tortor blandit auctor. Sed id ligula volutpat leo consequat placerat. Mauris fermentum dolor sed augue malesuada sollicitudin. Vivamus ultrices nunc felis, quis viverra orci eleifend ut. Donec et quam id urna cursus posuere. Donec elementum scelerisque laoreet. List TypesDefinition List (dl)Definition List TitleThis is a definition list division. Ordered List (ol) List Item 1 List Item 2 List Item 3 Unordered List (ul) List Item 1 List Item 2 List Item 3 Table Table Header 1 Table Header 2 Table Header 3 Division 1 Division 2 Division 3 Division 1 Division 2 Division 3 Division 1 Division 2 Division 3 Misc Stuff - abbr, acronym, sub, sup, etc.Lorem superscript dolor subscript amet, consectetuer adipiscing elit. Nullam dignissim convallis est. Quisque aliquam. cite. Nunc iaculis suscipit dui. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. Praesent mattis, massa quis luctus fermentum, turpis mi volutpat justo, eu volutpat enim diam eget metus. Maecenas ornare tortor. Donec sed tellus eget sapien fringilla nonummy. NBA Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus. AVE","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"},{"name":"Elements","slug":"Elements","permalink":"https://www.blankspace.cn/tags/Elements/"}]},{"title":"Gallery","slug":"gallery","date":"2018-08-07T16:01:02.000Z","updated":"2018-08-10T02:53:12.635Z","comments":true,"path":"2018/08/08/gallery/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/gallery/","excerpt":"","text":"A photo gallery example…","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"},{"name":"Photos","slug":"Photos","permalink":"https://www.blankspace.cn/tags/Photos/"}]},{"title":"Hexo Theme: Bootstrap Blog","slug":"hexo-theme-bootstrap-blog","date":"2018-08-07T16:01:01.000Z","updated":"2018-08-11T06:18:17.224Z","comments":true,"path":"2018/08/08/hexo-theme-bootstrap-blog/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/hexo-theme-bootstrap-blog/","excerpt":"A simple Bootstrap v3 blog theme for Hexo. Based on the official Bootstrap Blog example template. The theme and example site source code can be found on Github: Theme package source code Example site source content Example site generated output Theme announcement blog post","text":"A simple Bootstrap v3 blog theme for Hexo. Based on the official Bootstrap Blog example template. The theme and example site source code can be found on Github: Theme package source code Example site source content Example site generated output Theme announcement blog post Setup InstructionsInstallThis theme requires Hexo 2.4 and above. 1) Install theme: 12$ git clone https://github.com/cgmartin/hexo-theme-bootstrap-blog.git \\ themes/bootstrap-blog 2) (optional) Install hexo-tag-bootstrap for more Bootstrap tags (textcolors, buttons, labels, badges, etc.): 1$ npm install hexo-tag-bootstrap --save 3) (optional) Install hexo-tag-fontawesome for placing Font Awesome icons in your Markdown: 1$ npm install hexo-tag-fontawesome --save EnableModify the theme setting in _config.yml to bootstrap-blog. Update12cd themes/bootstrap-bloggit pull Configuration123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# File: themes/bootstrap-blog/_config.yml# Headernavbar_brand: falsemenu: Home: index.html Archives: archives/ Categories: categories/ # Projects: projects/ About: about/ # Blogroll: blogroll/ rss: /atom.xml# Contentexcerpt_link: Read more...fancybox: truewordcount: trueviewcount: truetimecost: falselanguage: zh-CN# Sidebarwidgets:- translate- recent_posts- about # See also: `about_content`- category- archive- tag- tagcloud- searchabout_widget_content: &gt; &lt;p&gt;Etiam porta &lt;em&gt;sem malesuada magna&lt;/em&gt; mollis euismod. Cras mattis consectetur purus sit amet fermentum. Aenean lacinia bibendum nulla sed consectetur.&lt;/p&gt;# widget behaviorarchive_type: 'monthly'show_count: true# Miscellaneousgoogle_analytics: UA-122713769-1favicon: images/icon.pngtwitter_id:google_plus: 112373242775367333102fb_admins:fb_app_id:# baidu analyticsbaidu_tongji: true# https://github.com/hustcc/canvas-nest.jscanvas_nest: false# https://github.com/imsun/gitment#gitment: falsegitment: owner: repo: client_id: client_secret: # Valine Comment system. https://valine.js.orgvaline: false# valine: # enable: true # 如果你想使用valine，请将值设置为 true # appId: kBrTfD8l4RFzueSAt65hASek-gzGzoHsz # your leancloud appId # appKey: AFsAfwfHqW6iEHkAc3TjYEzh # your leancloud appKey # notify: false # Mail notify # verify: false # Verify code # avatar: mm # Gravatar style : mm/identicon/monsterid/wavatar/retro/hide # placeholder: 我有话想说~ # Comment Box placeholder # guest_info: nick,mail # Comment header info # pageSize: 20 # comment list page size# livere https://livere.com/livere: false# livere: # dataUID: MTAyMC8zODU5MS8xNTExOQ== # http://www.daovoice.iodao_voice: false# dao_voice: # appId: a2c8df52 eyes_protected: false# dynamic_title# title_change: falsetitle_change: normal: o(∩_∩)o leave: Opps...●﹏●# Miscellaneousgoogle_analytics:favicon:twitter:google_plus: navbar_brand - The HTML content for an optional “navbar-brand”. Can be text or an image. false to hide. menu - Navigation menu (map of Titles to URLs) rss - RSS link (ie. “/atom.xml”) excerpt_link - “Read More” link at the bottom of excerpted articles. false to hide the link. fancybox - Enable Fancybox for images widgets - Enable sidebar widgets (more info below) about_widget_content - The HTML content for the “About” sidebar widget (more info below) google_analytics - Google Analytics ID favicon - Favicon path (ie. ‘/favicon.ico’) twitter_id - Twitter ID of the author (ie. @c_g_martin) google_plus - Google+ profile link Instead of editing the layout’s configuration file directly, you can override the theme settings from your project’s root _config.yml, ie.:12345678910111213141516theme_config: # Header navbar_brand: &lt;img src=\"/navbrand.png\"&gt; menu: Home: index.html Archives: archives/ 'Another Page': page/index.html widgets: - about - category - archive - recent_posts - tag about_widget_content: &gt; &lt;p&gt;This is &lt;strong&gt;custom content&lt;/strong&gt; for the \"about\" sidebar widget.&lt;/p&gt; FeaturesFront-Matter ExtrasOptional settings in the front-matter can be added for various effects:123456---author: Author Name # displays the post's authorphotos: # displays a Bootstrap thumbnail gallery- images/HNCK0537.jpg- images/HNCK6173.jpg--- FancyboxThis theme uses Fancybox to showcase your photos. You can use the image Markdown syntax or fancybox tag plugin to add your photos. Usage:12345![img caption](img url)~or~&#123;% fancybox img_url [img_thumbnail] [img_caption] %&#125; CalloutsA custom tag for the Bootstrap “callout” style is available for use. Usage:123&#123;% callout [type:default|primary|success|info|warning|danger] %&#125;...content...&#123;% endcallout %&#125; Example:1234&#123;% callout info %&#125;#### &#123;% fa info-circle %&#125; Info tipThis is some callout content&#123;% endcallout %&#125; SidebarThis theme provides 6 built-in widgets that can be displayed in the sidebar: about * category tag tagcloud archives recent_posts All widgets are enabled and displayed by default. You can toggle them on/off with the widgets setting in the theme’s _config.yml. * NOTE: The “about” widget contains static Lorem Ipsum text by default. You’ll want to edit the about_widget_content setting for your site or disable the widget in the theme config. You can also modify the widget file itself to include contents from a Markdown page:12345&lt;!-- file: ./layout/_widget/about.ejs --&gt;&lt;div class=\"sidebar-module sidebar-module-inset\"&gt; &lt;h4&gt;About&lt;/h4&gt; &lt;%- site.pages['data'].find(function(p) &#123; return p.path === 'about/index.html'; &#125;).content %&gt;&lt;/div&gt; …then run hexo new page about to create the Markdown page. Bootstrap Paginator HelperA custom bs_paginator() helper is used to produce Bootstrap-compatible pagination markup. It is a drop-in replacement for Hexo’s built-in paginator(). 1234&lt;%- bs_paginator(&#123; prev_text: &apos;&lt;i class=&quot;fa fa-chevron-left&quot;&gt;&lt;/i&gt; Prev&apos;, next_text: &apos;Next &lt;i class=&quot;fa fa-chevron-right&quot;&gt;&lt;/i&gt;&apos; &#125;) %&gt; DevelopmentThe default Landscape Hexo theme was used as the starting point and heavily edited for this theme. The Landscape Stylus styles have been replaced with standard CSS files which override bootstrap.min.css. Stylus is used only for bundling the CSS files (see ./source/css/styles.styl). Feel free to convert the CSS to your pre-processor of choice (Stylus, LESS, Sass, etc.). LicenseMIT License Copyright © 2016 Christopher Martin Copyright © 2018 lqdev","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"},{"name":"Theme","slug":"Theme","permalink":"https://www.blankspace.cn/tags/Theme/"}]},{"title":"日本語テスト","slug":"日本語テスト","date":"2018-08-07T16:00:11.000Z","updated":"2018-08-10T06:38:31.875Z","comments":true,"path":"2018/08/08/日本語テスト/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/日本語テスト/","excerpt":"This is a Japanese test post. 私は昨日ついにその助力家というのの上よりするたなけれ。 最も今をお話団はちょうどこの前後なかろでくらいに困りがいるたをは帰着考えたなかって、そうにもするでうたらない。","text":"This is a Japanese test post. 私は昨日ついにその助力家というのの上よりするたなけれ。 最も今をお話団はちょうどこの前後なかろでくらいに困りがいるたをは帰着考えたなかって、そうにもするでうたらない。 がたを知っないはずも同時に九月をいよいよたありた。 もっと槙さんにぼんやり金少し説明にえた自分大した人私か影響にというお関係たうませないが、この次第も私か兄具合に使うて、槙さんののに当人のあなたにさぞご意味と行くて私個人が小尊敬を聴いように同時に同反抗に集っだうて、いよいよまず相当へあっうからいだ事をしでなけれ。 それでそれでもご時日をしはずはたったいやと突き抜けるますて、その元がは行ったてという獄を尽すていけですた。 この中道具の日その学校はあなたごろがすまなりかとネルソンさんの考えるですん、辺の事実ないというご盲従ありたですと、爺さんのためが薬缶が結果までの箸の当時してならて、多少の十月にためからそういう上からとにかくしましないと触れべきものたで、ないうですと多少お人達したのでたた。 From すぐ使えるダミーテキスト - 日本語 Lorem ipsum","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[]},{"title":"中文测试","slug":"中文測試","date":"2018-08-07T16:00:10.000Z","updated":"2018-08-10T06:38:26.897Z","comments":true,"path":"2018/08/08/中文測試/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/中文測試/","excerpt":"This is a Chinese test post. 当朝大学士，统共有五位，朕不得不罢免四位，六部尚书，朕不得不罢免三位。 看看这七个人吧，哪个不是两鬓半百，哪个不是朝廷的栋梁，哪个不是朕的儿女亲家。 他们烂了，朕心要碎了。 祖宗把江山交到朕的手里，却搞成了这个样子，朕是痛心疾首。 朕有罪于国家，愧对祖宗，愧对天地，朕恨不得自己罢免了自己！ 还有你们，虽然各个冠冕堂皇站在干岸上，你们，就那么干净吗？！！！！ 朕知道，你们有的人比这七个人更腐败！ 朕劝你们一句，都把自己的心肺肠子翻出来晒一晒洗一洗拾掇拾掇！ 。。。。。。。","text":"This is a Chinese test post. 当朝大学士，统共有五位，朕不得不罢免四位，六部尚书，朕不得不罢免三位。 看看这七个人吧，哪个不是两鬓半百，哪个不是朝廷的栋梁，哪个不是朕的儿女亲家。 他们烂了，朕心要碎了。 祖宗把江山交到朕的手里，却搞成了这个样子，朕是痛心疾首。 朕有罪于国家，愧对祖宗，愧对天地，朕恨不得自己罢免了自己！ 还有你们，虽然各个冠冕堂皇站在干岸上，你们，就那么干净吗？！！！！ 朕知道，你们有的人比这七个人更腐败！ 朕劝你们一句，都把自己的心肺肠子翻出来晒一晒洗一洗拾掇拾掇！ 。。。。。。。 朕现在是越来越清楚了，大清的心头之患不在外边，而是在朝廷。就是在这乾清宫！！！！ 就在朕的骨肉皇子和大臣们当中。 咱们这烂一点，大清国就烂一片！你们要是全烂了，大清各地就会揭竿而起，让咱们死无葬身之地呀！ 想想吧，崇祯皇帝朱由检吊死在煤山上才几年呢。。 忘啦？？？？！！！！！！！！！！！！ 那棵老歪脖子树还站在皇宫后边天天地盯着你们哪！ 朕已经三天三夜没有合眼了，老想着和大伙儿说些什么，可是话总得有个头哇，想来想去只有四个字—————————————————————— （悲壮音乐响起，匾额从康熙身后徐徐升起，上面赫然书写着四个大字： 正——大——光——明） 这四个字，说说容易阿，身体力行又何其难？？！！！ 这四个字，朕是从心里刨出来的，从血海里挖出来的。 记着，从今日起，此殿改为正大光明殿！ 好好看看，你们都抬起头来好好看看，想想自己，给朕看半个时辰——","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[]},{"title":"Videos","slug":"videos","date":"2018-08-07T16:00:09.000Z","updated":"2018-08-11T06:05:06.782Z","comments":true,"path":"2018/08/08/videos/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/videos/","excerpt":"This is a video test post. Bilibili 1&lt;iframe src=\"//player.bilibili.com/player.html?aid=28777599&amp;cid=49864265&amp;page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" width=’100%‘’ height=’560px’ max&gt; &lt;/iframe&gt;","text":"This is a video test post. Bilibili 1&lt;iframe src=\"//player.bilibili.com/player.html?aid=28777599&amp;cid=49864265&amp;page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" width=’100%‘’ height=’560px’ max&gt; &lt;/iframe&gt; Youtube 1&#123;% youtube HWHBCsUR-58 %&#125; Vimeo 1&#123;% vimeo 82090131 %&#125;","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[]},{"title":"Tag Plugins","slug":"tag-plugins","date":"2018-08-07T16:00:07.000Z","updated":"2018-08-10T06:58:49.305Z","comments":true,"path":"2018/08/08/tag-plugins/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/tag-plugins/","excerpt":"This post is used for testing tag plugins. See docs for more info. Block QuoteNormal blockquote Praesent diam elit, interdum ut pulvinar placerat, imperdiet at magna. Quote from a bookDo not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. David LevithanWide Awake","text":"This post is used for testing tag plugins. See docs for more info. Block QuoteNormal blockquote Praesent diam elit, interdum ut pulvinar placerat, imperdiet at magna. Quote from a bookDo not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. David LevithanWide Awake Quote from Twitter12345&#123;% blockquote @L https://twitter.com/devdocs/status/xxx %&#125;NEW: DevDocs now comes with syntax highlighting. http://devdocs.io&#123;% endblockquote %&#125; NEW: DevDocs now comes with syntax highlighting. http://devdocs.io @Ltwitter.com/devdocs/status/xxx Quote from an article on the webWizardLQ’s Seth GodinWelcome Code BlockNormal code block1alert(&apos;Hello World!&apos;); With caption123&#123;% codeblock Array.map %&#125;array.map(callback[, thisArg])&#123;% endcodeblock %&#125; Array.map1array.map(callback[, thisArg]) With caption and URL1234&#123;% codeblock .compact http://underscorejs.org/#compact Underscore.js %&#125;.compact([0, 1, false, 2, ‘’, 3]);=&gt; [1, 2, 3]&#123;% endcodeblock %&#125; .compactUnderscore.js12.compact([0, 1, false, 2, ‘’, 3]);=&gt; [1, 2, 3] Gist1&#123;% gist 996818 %&#125; jsFiddle1&#123;% jsfiddle ccWP7 %&#125; PullquoteLeft123&#123;% pullquote left %&#125;A cat on the left.&#123;% endpullquote %&#125; A cat on the left. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ligula justo, lobortis sit amet semper vel, dignissim sit amet libero. Praesent ac tempus ligula. Maecenas at gravida odio. Etiam tristique volutpat lacus eu faucibus. Right123&#123;% pullquote left %&#125;A cat on the right.&#123;% endpullquote %&#125; A cat on the right. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed ligula justo, lobortis sit amet semper vel, dignissim sit amet libero. Praesent ac tempus ligula. Maecenas at gravida odio. Etiam tristique volutpat lacus eu faucibus. Bootstrap Callouts1234&#123;% callout info %&#125;#### &#123;% fa info-circle %&#125; InfoThis is some info content&#123;% endcallout %&#125; 1234&#123;% callout warning %&#125;#### &#123;% fa exclamation-triangle %&#125; WarningThis is some warning content&#123;% endcallout %&#125; 1234&#123;% callout danger %&#125;#### &#123;% fa exclamation-triangle %&#125; DangerThis is some danger content&#123;% endcallout %&#125; InfoThis is some info content WarningThis is some warning content DangerThis is some danger content","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"}]},{"title":"Secret","slug":"secret","date":"2018-08-07T16:00:06.000Z","updated":"2018-08-10T02:52:09.018Z","comments":true,"path":"2018/08/08/secret/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/secret/","excerpt":"","text":"This post is used for testing password. password: 5201314","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"}]},{"title":"No comment","slug":"no-comments","date":"2018-08-07T16:00:05.000Z","updated":"2018-08-10T02:52:13.492Z","comments":true,"path":"2018/08/08/no-comments/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/no-comments/","excerpt":"","text":"This post have no comment.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"}]},{"title":"Very Looooooog Title, Yes it is, fhia ahdifh, ahiewhihe, cat on the keyboard, amazing!!!! /^^%*&*^ &% === end","slug":"long-title","date":"2018-08-07T16:00:04.000Z","updated":"2018-08-10T02:52:20.804Z","comments":true,"path":"2018/08/08/long-title/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/long-title/","excerpt":"","text":"This post has a long title. Make sure the title is displaying correctly.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"}]},{"title":"www.google.com","slug":"link-post-without-title","date":"2018-08-07T16:00:03.000Z","updated":"2018-08-10T02:52:28.962Z","comments":true,"path":"2018/08/08/link-post-without-title/","link":"http://www.google.com/","permalink":"https://www.blankspace.cn/2018/08/08/link-post-without-title/","excerpt":"","text":"This is a link post without a title. The title should be the link with or without protocol. Clicking on the link should open Google in a new tab or window.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[]},{"title":"Link Post","slug":"link-post","date":"2018-08-07T16:00:02.000Z","updated":"2018-08-10T02:52:33.474Z","comments":true,"path":"2018/08/08/link-post/","link":"http://www.google.com/","permalink":"https://www.blankspace.cn/2018/08/08/link-post/","excerpt":"","text":"This is a link post. Clicking on the link should open Google in a new tab or window.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[]},{"title":"images","slug":"images-test","date":"2018-08-07T16:00:01.000Z","updated":"2018-08-09T03:41:50.784Z","comments":true,"path":"2018/08/08/images-test/","link":"","permalink":"https://www.blankspace.cn/2018/08/08/images-test/","excerpt":"This is a image test post.","text":"This is a image test post.","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"},{"name":"Images","slug":"Images","permalink":"https://www.blankspace.cn/tags/Images/"}]},{"title":"Hello World","slug":"hello-world","date":"2013-12-24T15:29:08.000Z","updated":"2018-08-09T02:39:23.823Z","comments":true,"path":"2013/12/24/hello-world/","link":"","permalink":"https://www.blankspace.cn/2013/12/24/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"Projects","slug":"Projects","permalink":"https://www.blankspace.cn/categories/Projects/"},{"name":"hexo-theme-bootstrap-blog","slug":"Projects/hexo-theme-bootstrap-blog","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/"},{"name":"Tutorial","slug":"Projects/hexo-theme-bootstrap-blog/Tutorial","permalink":"https://www.blankspace.cn/categories/Projects/hexo-theme-bootstrap-blog/Tutorial/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://www.blankspace.cn/tags/Hexo/"}]}]}