<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">


<title>WizardLQ’s | 魔法师の小茶馆</title>
<!-- Bootstrap core CSS -->
<link rel="stylesheet" href="css/bootstrap.min.css">

</head>

<body>

	<nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark">
  <a class="navbar-brand" href="#">导航栏BUG</a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
	<span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarCollapse">
	<ul class="nav navbar-nav mr-auto">
	  <li class="nav-item active">
		<a class="nav-link" href="#">Home <span class="sr-only">(current)</span></a>
	  </li>
	  <li class="nav-item">
		<a class="nav-link" href="#">Link</a>
	  </li>
	  <li class="nav-item">
		<a class="nav-link disabled" href="#">Disabled</a>
	  </li>
	</ul>

  </div>
</nav>




	<div class="container">

<div class="blog-header">
 <h1 class="blog-title">WizardLQ’s | 魔法师の小茶馆</h1>
  
    <p class="lead blog-description">Keep moving, never give up. | 锲而不舍，金石可镂.</p>
  
</div>



<div class="row">
    <div class="col-sm-9 blog-main">
	
	
	  
		<article id="post-MathJax-basic-tutorial" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/14/MathJax-basic-tutorial/">MathJax basic tutorial and quick reference</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/14/MathJax-basic-tutorial/" class="article-date"><time datetime="2018-08-13T23:46:53.000Z" itemprop="datePublished">2018-08-14</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Math/">Math</a> / <a class="article-category-link" href="categories/Math/MathJax/">MathJax</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
			<p>更多内容，请看<a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference?page=2&amp;tab=votes#tab-top" target="_blank" rel="noopener">这里</a>.本文是本人对其中部分的翻译，若有疏漏，还请联系作者更正。</p>
<p>(德语版本: <a href="https://www.mathelounge.de/509545/mathjax-latex-basic-tutorial-und-referenz-deutsch" target="_blank" rel="noopener">Deutsch: MathJax: LaTeX Basic Tutorial und Referenz</a>)</p>
<h2 id="基础部分"><a href="#基础部分" class="headerlink" title="基础部分"></a>基础部分</h2><ol>
<li><p><strong>对于行内公式，使用<code>$...$</code>括起来。对于整行显示公式，使用<code>$$...$$</code>括起来。</strong></p>
<p>比方说，键入<code>$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$</code>可以在一行内嵌入公式$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$ （这就是inline模式了），或者我们键入<code>$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$</code>就可以显示：</p>
<script type="math/tex; mode=display">
\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}</script><p>（这就是display模式了）。</p>
</li>
<li><p>对于<strong>希腊字母</strong>，使用诸如<code>\alpha</code>, <code>\beta</code>, …, <code>\omega</code> :$\alpha, \beta, …, \omega$.大写希腊字母，使用诸如<code>\Gamma</code>, <code>\Delta</code>, …, <code>\Omega</code>:  $\Gamma, \Delta, …, \Omega$.</p>
</li>
<li><p>对于<strong>上标还有下标</strong>，使用<code>^</code>和<code>_</code>.比如<code>x_i^2</code>:$x_i^2 $,<code>\log_2 x</code>:$\log_2 x$.</p>
</li>
<li><p><strong>组（Group）</strong>.被一对<code>{</code>…<code>}</code>包裹的部分就是一个组，里面可以是一个单一的字符，也可以是一串公式。比如输入<code>10^10</code>,得到的是$10^10$,而不是我们想要的<code>10^{10}</code>:$10^{10}$.使用花括号来对公式进行划界，否则例如:<code>x^5^6</code>就是一种错误的表示。<code>{x^y}^z</code>得到${x^y}^z$,而<code>x^{y^z}</code>得到$x^{y^z}$.还有比方说<code>x_i^2</code>:$x_i^2$,而<code>x_{i^2}</code>:$x_{i^2}$.</p>
</li>
<li><p><strong>括号</strong>。普通<code>()[]</code>分别生成圆括号和方括号。使用<code>\{</code>和<code>\}</code>来生成花括号：$\{ \}$.但是这样的括号并不能随着内部公式的高度来调整，所以写<code>(\frac{\sqrt x}{y^3})</code>时括号就显得有点装不下了$(\frac{\sqrt{x}}{y^3})$.使用<code>\left(</code>和<code>\right)</code>能使得括号根据括起来的公式自动调整<code>\left( \frac{\sqrt x)}{y^3} \right)</code>:$\left( \frac{\sqrt x}{y^3} \right)$.</p>
<p><code>\left</code>和<code>\right</code>可以应用于下列括号：<code>(</code>和<code>)</code>$\left(x \right)$, <code>[</code>和<code>]</code>$\left[ x \right]$. <code>\{</code>和<code>\}</code>，如<code>\left\{x\right\}</code>, $\left\{x\right\}$. <code>|</code>如<code>\left| x \right|</code>$\left| x \right|$, <code>\vert</code>$\left\vert x \right\vert$,<code>\Vert</code>$\left\Vert x \right\Vert$, <code>\langle</code>和<code>\rangle</code>$\left\langle x \right\rangle$, <code>\lcei;</code>和<code>\rceil</code>$\left\lceil x \right\rceil$, <code>\lfloor</code>和<code>\rfloor</code>$\left\lfloor x \right\rfloor$, <code>\middle</code>可以用了增加额外的分界。此外有隐藏括号，用<code>.</code>来标记: <code>\left.\frac12\right\rbrace</code>显示$\left.\frac12\right\rbrace$.</p>
<p>手动调整括号大小<code>\Biggl(\biggl(\Bigl(\bigl(x\bigr)\Bigr)\biggl)\Biggl)</code>显示为：$\Biggl(\biggl(\Bigl(\bigl(x\bigr)\Bigr)\biggl)\Biggl)$.</p>
</li>
<li><p><strong>累加与积分（sum and integral）</strong><code>\sum</code>与<code>\int</code>表示; 上标为上界，下标为下界，所以例如<code>\sum_x^n</code>$\sum_1^n$.若界超过一个字符表示，不要忘记使用<code>{</code>…<code>}</code>, 例如<code>\sum_{i=0}^\infty i^2</code>$\sum_{i=0}^\infty i^2$. 类似的还有累乘<code>\prod</code>$\prod$, 积分<code>\int</code>$\int$, 并集<code>\bigcup</code>$\bigcup$, 交集<code>\bigcap</code>$\bigcap$, 二重积分<code>\iint</code>$\iint$, 三重积分<code>\iiint</code>$\iiint$.</p>
</li>
<li><p><strong>分数（fraction）</strong>，<a href="https://math.meta.stackexchange.com/q/12978/3111" target="_blank" rel="noopener">创建分数的三种方法</a>. <code>\frac ab</code>得到$\frac ab$;对于更复杂分子和分母使用<code>{</code>…<code>}</code>: <code>\frac{a+1}{b+1}</code>$\frac{a+1}{b+1}$. 如果分数分子分母实在是复杂，我们可能会使用<code>\over</code>，将一个组分开：<code>{a+1 \over b+1}</code>${a+1 \over b+1}$. 对于连续的多个分数<code>\cfrac{a}{b}</code>$\cfrac{a}{b}$能派上用场。更多细节，见[后文]。</p>
</li>
<li><p><strong>字体</strong></p>
<ul>
<li><p>“Blackboard bold”,使用<code>\mathbb</code>或者<code>Bbb</code>：$\Bbb{ABCDEFGHIJKLMNOPQRSTUVWXYZ}$$\mathbb{abcdefghijklmnopqrstuvwxyz} $</p>
</li>
<li><p>“Boldface”,使用<code>\mathbf</code>：$\mathbf {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$$\mathbf {abcdefghijklmnopqrstuvwxyz} $</p>
</li>
<li><p>“Typewriter”(打字机体),使用<code>\mathtt</code>：$\mathtt {ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ}$$\mathtt{abcdefghijklmnopqrstuvwxyz}$</p>
</li>
<li><p>“Roman”(罗马体), 使用<code>\mathrm</code>: $\mathrm ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ  $$\mathrm{abcdefghijklmnopqrstuvwxyz}$</p>
</li>
<li><p>“Sans-serif”, 使用<code>\mathsf</code>:$\mathsf {ABCDEFGHIJKLMNOPQRSTUVWXYZ}$$\mathsf abcdefghijklmnopqrstuvwxyz $</p>
</li>
<li><p>“Calligraphic”, 使用<code>\mathcal</code>:$\mathcal {ABCDEFGHIJKLMNOPQRSTUVWXYZ }$</p>
</li>
<li><p>“Script”(手写体), 使用<code>\mathscr</code>: $\mathscr {ABCDEFGHIJKLMNOPQRSTUVWXYZ}  $</p>
</li>
<li><p>“Fraktur”(古德语体), 使用<code>\mathfrak</code>: </p>
<p>$\mathfrak{ABCDEFGHIJKLMNOPQRSTUVWXYZ  }$$\mathfrak {abcdefghijklmnopqrstuvwxyz} $</p>
</li>
</ul>
</li>
<li><p><strong>开方</strong>，<code>\sqrt</code>,能根据根号内容调整：<code>\sqrt{x^3}</code>$\sqrt{x^3}$;$\sqrt[3]{\frac xy}$.对于更复杂的表达使用如<code>{...}^{1/2}</code>表示，${x}^{1/2}$和$\sqrt{x}$一样.</p>
</li>
<li><p><strong>特殊的函数</strong>，诸如，极限“lim”, 正弦“sin”, 最大值“max”, 自然对数”ln”等通常以罗马字体代替斜体来显示。比如使用<code>\lim</code>,<code>\sin</code>等，来显示<code>\sin</code>$\sin x$, 而不是<code>sin x</code>$sin x$.使用下标来对<code>\lim</code>进行标注：<code>\lim_{x\to0}</code>显示$\lim_{x\to0}$.</p>
</li>
<li><p>此外有很多的<strong>特别的标记符号</strong>，部分符号列表参考<a href="https://pic.plover.com/MISC/symbols.pdf" target="_blank" rel="noopener">这里</a>，完整的符号列表参考<a href="https://www.ctan.org/tex-archive/info/symbols/comprehensive/symbols-a4.pdf" target="_blank" rel="noopener">这里</a>.最常用的如下：</p>
<ul>
<li><strong>比较</strong>，<code>\lt \gt \le \leq \leqq \leqslant \ge \geq \geqq\geqslant \neq</code>$\lt \gt \le \leq \leqq \leqslant \ge \geq \geqq\geqslant \neq$. 可以使用<code>\not</code>给任何符号加上一道斜线<code>\not\lt</code>$\not\lt$, 通常这样效果并不佳。</li>
<li><code>+ - \times \div \pm \mp</code>$+-\times \div \pm \mp $.<code>\cdot</code>中心点，$x\cdot y$</li>
<li><strong>集合</strong>，<code>\cup \cap \setminus \subset \subseteq \subsetneq \supset \in \notin \emptyset \varnothing</code>, $\cup \cap \setminus \subset \subseteq \subsetneq \supset \in \notin \emptyset \varnothing $</li>
<li><code>{n+1 \choose 2k}</code>或<code>\binom{n+1}{2k}</code>$\binom{n+1}{2k} $</li>
<li><strong>箭头</strong>，<code>\to \rightarrow \leftarrow \Rightarrow \Leftarrow \mapsto</code>$\to \rightarrow \leftarrow \Rightarrow \Leftarrow \mapsto $</li>
<li><strong>逻辑</strong>，<code>\land \lor \lnot \forall \exists \top \bot \vdash \vDash</code>$\land \lor \lnot \forall \exists \top \bot \vdash \vDash $</li>
<li><strong>形状</strong>，<code>\star \ast \oplus \circ \bullet</code>$\star \ast \oplus \circ \bullet $</li>
<li><code>\approx \sim \simeq \cong \equiv \prec \lhd</code>$\approx \sim \simeq \cong \equiv \prec \lhd $</li>
<li><code>\infty \aleph_0</code>$\infty \aleph_0 $<code>\nabla \partial</code>$\nabla \partial$<code>\Im \Re</code>$\Im \Re$</li>
<li>对于取模相等使用<code>\pmod</code>例如：<code>a\equiv b\pmod n</code>$a\equiv b\pmod n$</li>
<li>$a_1, a_2, \ldots a_n$使用<code>\ldots</code>; $a_1+a_2+\cdots+a_n$使用<code>\cdots</code></li>
<li>一些希腊字母有多种形式：比如<code>\epsilon</code>和<code>\varepsilon</code>$\epsilon \varepsilon$, <code>\phi \varphi</code>$\phi \varphi$,等。手写体小写字母l<code>\ell</code>$\ell$</li>
</ul>
<p><a href="http://detexify.kirelabs.org/classify.html" target="_blank" rel="noopener">Detexif</a>能将你所绘制的图形识别成与之接近的$\TeX$符号。此外更多详情欢迎查看MathJax.org以及<a href="http://docs.mathjax.org/en/latest/tex.html#supported-latex-commands" target="_blank" rel="noopener"> list of currently supported $\LaTeX$ commands </a>和<a href="http://www.onemathematicalcat.org/MathJaxDocumentation/TeXSyntax.htm" target="_blank" rel="noopener">$\TeX$Commands Available in MathJax</a>.</p>
</li>
<li><p><strong>空格</strong>。在公式之间添加更多的空格数，不会增加显示的空格宽度，<code>a b</code>和<code>a        b</code>都会显示$a        b$.增加更多空格：<code>\,</code>增加小空格$a\,b$;<code>\;</code>增加更宽一点的空格$a\;b$;更宽的空格使用<code>\quad</code>$a\quad b$和<code>\qquad</code>$a\qquad b$.</p>
<p>增加纯文本，使用<code>\text{...}</code>:$\left\{ x\in s| \text{ x is exatra large}\right\}$.在<code>\text{...}</code>中亦可使用<code>$...$</code>.</p>
</li>
<li><p><strong>音标和变音符号</strong>，<code>\hat</code>对单个标记加<code>...帽</code>，对公式加<code>帽</code>使用<code>\widehat</code>$\widehat{xyz}$,不过如果太宽的话看起来就有点<code>哈巴</code>了。类似的<code>\bar</code>$\bar{x}$, <code>\overline</code>$\overline{xyz}$, <code>\vec</code>$\vec x$以及<code>\overrightarrow$\overrightarrow{xyz}$</code>还有<code>\overleftrightarrow</code>$\overleftrightarrow{xyz}$. 对于点符号，例如$\frac{d}{dx}x\dot x=\dot x^2+x\ddot x$分别使用的是<code>\dot</code>和<code>\ddot</code>.</p>
</li>
<li><p><strong>特殊符号转义</strong>，使用<code>\</code>转义符：<code>\$</code>$$$, <code>\{</code>$\{$, <code>\_</code>$_$等。假如我们想要输出<code>\</code>符号本身，使用<code>\backslash</code>$\backslash$，而<code>\\</code>被用于创建新的一行。</p>
</li>
</ol>
<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><ol>
<li><p>使用<code>\begin{matrix}...\end{matrix}</code>表示矩阵。在<code>\begin</code>和<code>\end</code>之间放置矩阵元素。矩阵每一行用<code>\\</code>分隔，元素之间使用<code>&amp;</code>来分隔。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">    \begin&#123;matrix&#125;</span><br><span class="line">    1 &amp; x &amp; x^2 \\</span><br><span class="line">    4 &amp; y &amp; y^2 \\</span><br><span class="line">    7 &amp; z &amp; z^2 \\</span><br><span class="line">    \end&#123;matrix&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<p>得到：</p>
</li>
</ol>
<script type="math/tex; mode=display">
\begin{matrix}
   1&x&x^2\\
   4&y&y^2\\
   7&z&z^2\\
   \end{matrix}</script><p>​    MathJax能够自动调整行和列的大小来自适应。</p>
<ol>
<li><p><strong>给矩阵增加括号</strong>使用上一节中的第5部分，或者将<code>matrix</code>替换成<code>pmatrix</code>$\begin{pmatrix}1&amp;2\\3&amp;4\\ \end{pmatrix}$, <code>bmatrix</code>$\begin{bmatrix}1&amp;2\\3&amp;4\\ \end{bmatrix}$, <code>Bmatrix</code>$\begin{Bmatrix}1&amp;2\\3&amp;4\\ \end{Bmatrix}$, <code>vmatrix</code>$\begin{vmatrix}1&amp;2\\3&amp;4\\ \end{vmatrix}$, <code>Vmatrix</code>$\begin{Vmatrix}1&amp;2\\3&amp;4\\ \end{Vmatrix}$.</p>
</li>
<li><p>使用<code>\cdots</code>$\cdots$, <code>\ddots</code>$\ddots$, <code>\vdots</code>$\vdots$, 来表示矩阵中被忽略的条目:</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="formula">$$</span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">begin</span><span class="string">&#123;pmatrix&#125;</span></span></span></span><br><span class="line"><span class="formula">1&amp;a_1&amp;a_1^2&amp;<span class="tag">\<span class="name">cdots</span></span>&amp;a_1^n<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula">1&amp;a_2&amp;a_2^2&amp;<span class="tag">\<span class="name">cdots</span></span>&amp;a_2^n<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">vdots</span></span>&amp;<span class="tag">\<span class="name">vdots</span></span>&amp;<span class="tag">\<span class="name">vodts</span></span>&amp;ddots&amp;<span class="tag">\<span class="name">vdots</span></span><span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula">1&amp;a_m&amp;a_m^2&amp;<span class="tag">\<span class="name">cdots</span></span>&amp;a_m^n<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">end</span><span class="string">&#123;pmatrix&#125;</span></span></span></span><br><span class="line"><span class="formula">$$</span></span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
\begin{pmatrix}
1&a_1&a_1^2&\cdots&a_1^n\\
1&a_2&a_2^2&\cdots&a_2^n\\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1&a_m&a_m^2&\cdots&a_m^n\\
\end{pmatrix}</script></li>
</ol>
<ol>
<li><p>对于水平方向“增广“矩阵，用方括号或者圆括号将与之对应格式的表格括起来；详情见后文的<a href="_">数组</a>一节。举例如下：</p>
<script type="math/tex; mode=display">
\left[
\begin{array}{cc|c}
1&2&3\\
4&5&6
\end{array}
\right]</script><p>表达式为:</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="formula">$$</span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">left</span><span class="string">[</span></span></span></span><br><span class="line"><span class="formula"><span class="tag"><span class="string">\begin&#123;array&#125;&#123;cc|c&#125;</span></span></span></span><br><span class="line"><span class="formula"><span class="tag"><span class="string">1&amp;2&amp;3\\</span></span></span></span><br><span class="line"><span class="formula"><span class="tag"><span class="string">4&amp;5&amp;6</span></span></span></span><br><span class="line"><span class="formula"><span class="tag"><span class="string">\end&#123;array&#125;</span></span></span></span><br><span class="line"><span class="formula"><span class="tag"><span class="string">\right]</span></span></span></span><br><span class="line"><span class="formula">$$</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>对于垂直方向的“增广”矩阵，使用<code>\hline</code>，例如：</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
a&b\\
c&d\\
\hline
1&0\\
0&1
\end{pmatrix}</script><p>表达式为:</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="formula">$$</span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">begin</span><span class="string">&#123;pmatrix&#125;</span></span></span></span><br><span class="line"><span class="formula">a&amp;b<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula">c&amp;d<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">hline</span></span></span></span><br><span class="line"><span class="formula">1&amp;0<span class="tag">\<span class="name">\</span></span></span></span><br><span class="line"><span class="formula">0&amp;1</span></span><br><span class="line"><span class="formula"><span class="tag">\<span class="name">end</span><span class="string">&#123;pmatrix&#125;</span></span></span></span><br><span class="line"><span class="formula">$$</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>对于小行内矩阵，使用<code>\bigl(\begin{smallmatrix}...\end{smallmatrix}\bigr)</code>, 例如：<code>\bigl(\begin{smallmatrix}a&amp;b\\c&amp;d\end{smallmatrix}\bigr)</code>$\bigl(\begin{smallmatrix}a&amp;b\\c&amp;d\end{smallmatrix}\bigr)$.</p>
</li>
</ol>
			
			  <p class="article-more-link">
				<a class="btn btn-primary" href="2018/08/14/MathJax-basic-tutorial/#more">Read more...</a>
			  </p>
			
		  
		</div>

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/14/MathJax-basic-tutorial/" data-id="cjkyqovbm000d6sfi0nt305pi" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/MathJax/">MathJax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TeX/">TeX</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-activation-function" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/13/activation-function/">Activation function</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/13/activation-function/" class="article-date"><time datetime="2018-08-13T13:24:57.000Z" itemprop="datePublished">2018-08-13</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
			<p>Activation function（激活函数），是一类非线性函数，与之对应的就是线性函数。线性函数就是一条直线，形式化表示就是：</p>
<script type="math/tex; mode=display">
y=mx+b</script><p>$m$是直线的斜率，$y$通常是预测的值，$x$是输入的特征值，$b$是$y$轴的截距。</p>
<p>由于现实世界的复杂性，输入特征和预测之间的关系通常不是简单的线性关系，换言之就是非线性关系。然后就发现一些非线性函数，满足一定的性质（这个得看人家的论文），通过非线性函数多层的组合，可以来拟合非常复杂的非线性函数。</p>
<p>更过关于激活函数的介绍，请看<a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank" rel="noopener">这里</a>.</p>
<p>TensorFlow中更多激活函数，请看<a href="https://www.tensorflow.org/api_guides/python/nn#Activation_Functions" target="_blank" rel="noopener">TensorFlow Activation Function</a>.</p>
<h3 id="Binary-step"><a href="#Binary-step" class="headerlink" title="Binary step"></a>Binary step</h3><script type="math/tex; mode=display">
f(x) =
\begin{cases}
0,  & \text{for x $\lt$ 0} \\
1, & \text{for x $\geq$ 0}
\end{cases}</script><p>阶跃函数是理想的激活函数，但是不可导。</p>
<p>下面对几种常见的激活函数进行可视化。</p>
<h3 id="ReLu-rectified-linear-unit-ReLu"><a href="#ReLu-rectified-linear-unit-ReLu" class="headerlink" title="ReLu(rectified linear unit, ReLu)"></a>ReLu(rectified linear unit, ReLu)</h3><script type="math/tex; mode=display">
f(x) =
\begin{cases}
0,  & \text{for x $\lt$ 0} \\
x, & \text{for x $\geq$ 0}
\end{cases}</script><h3 id="Logistic-Sigmoid-or-Soft-step"><a href="#Logistic-Sigmoid-or-Soft-step" class="headerlink" title="Logistic(Sigmoid or Soft step)"></a>Logistic(Sigmoid or Soft step)</h3><script type="math/tex; mode=display">
f(x)=\sigma{(x)} = \frac{1}{1+e^{-x}}</script><h3 id="TanH"><a href="#TanH" class="headerlink" title="TanH"></a>TanH</h3><script type="math/tex; mode=display">
f(x)= \tanh{(x)}= \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script><h3 id="Softplus"><a href="#Softplus" class="headerlink" title="Softplus"></a>Softplus</h3><script type="math/tex; mode=display">
f(x)= \ln{(1+e^x)}</script><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><script type="math/tex; mode=display">
f_{i}(x)= \frac{e^{x_{i}}}{\sum_{j=1}^{J}e^{x_j}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf <span class="comment"># machine learning framwork</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># scientific computation</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># data visualization</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default(), tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x_data = np.linspace(<span class="number">-6</span>, <span class="number">6</span>, <span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">    y_relu = tf.nn.relu(x_data)</span><br><span class="line">    y_sigmoid = tf.nn.sigmoid(x_data)</span><br><span class="line">    y_tanh = tf.nn.tanh(x_data)</span><br><span class="line">    y_softplus = tf.nn.softplus(x_data)</span><br><span class="line">    y_softmax = tf.nn.softmax(x_data) <span class="comment"># softmax is a special kind of activation function, it is about probablity</span></span><br><span class="line"></span><br><span class="line">    y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax = sess.run([y_relu, y_sigmoid, y_tanh, y_softplus, y_softmax])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">    plt.subplot(<span class="number">221</span>)</span><br><span class="line">    plt.plot(x_data, y_relu, c=<span class="string">'red'</span>, label=<span class="string">'relu'</span>)</span><br><span class="line">    plt.ylim(<span class="number">-1</span>, <span class="number">6</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">222</span>)</span><br><span class="line">    plt.plot(x_data, y_sigmoid, c=<span class="string">'green'</span>, label=<span class="string">'sigmoid'</span>)</span><br><span class="line">    plt.ylim(<span class="number">-0.3</span>, <span class="number">1.5</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">223</span>)</span><br><span class="line">    plt.plot(x_data, y_tanh, c=<span class="string">'blue'</span>, label=<span class="string">'tanh'</span>)</span><br><span class="line">    plt.ylim(<span class="number">-1.2</span>, <span class="number">1.2</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.subplot(<span class="number">224</span>)</span><br><span class="line">    plt.plot(x_data, y_softplus, c=<span class="string">'yellow'</span>, label=<span class="string">'softplus'</span>)</span><br><span class="line">    plt.ylim(<span class="number">-0.3</span>, <span class="number">6</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/08/13/activation-function/output_1_0.png" alt="Activation Functions-1"></p>
			
			  <p class="article-more-link">
				<a class="btn btn-primary" href="2018/08/13/activation-function/#more">Read more...</a>
			  </p>
			
		  
		</div>

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/13/activation-function/" data-id="cjkyqovbz000o6sfi5zvqnmss" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/Activation/">Activation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-placeholder" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/13/placeholder/">Placeholder</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/13/placeholder/" class="article-date"><time datetime="2018-08-13T07:05:47.000Z" itemprop="datePublished">2018-08-13</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
		  
			<!-- Table of Contents -->

			
			
			<h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><p><strong>A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our operations and build our computation graph, without needing the data.</strong></p>
<p>顾名思义，占位符，没有数据先占位。是一种变量。之后按照<code>sess.run(***, feed_dict={input: **})</code>形式传输数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default(), tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x = tf.placeholder(tf.float32)</span><br><span class="line">    y = x**<span class="number">3</span></span><br><span class="line">    feed_dict = &#123;x:[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]&#125;</span><br><span class="line">    print(sess.run(y, feed_dict=feed_dict))</span><br></pre></td></tr></table></figure>
<pre><code>[ 1.  8. 27. 64.]
</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>placeholder</code> 与 <code>feed_dict={}</code> 总是一起出现的。placeholder允许没有数据先占位创建计算图，运行时通过<code>feed_dict</code>传入数据。</p>

			
		  
		</div>

		
		  

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/13/placeholder/" data-id="cjkyqovct001k6sfikhlh8c39" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/placeholder/">placeholder</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-Tensor-Control" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/13/Tensor-Control/">Tensor Control</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/13/Tensor-Control/" class="article-date"><time datetime="2018-08-13T02:06:14.000Z" itemprop="datePublished">2018-08-13</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
		  
			<!-- Table of Contents -->

			
			
			<h2 id="张量形状"><a href="#张量形状" class="headerlink" title="张量形状"></a>张量形状</h2><p>形状shape，用来描述张量的大小和数量。张量的形状表示为列表的形式，其中第i个元素表示维度i的大小，列表的长度标书张量的阶（维数）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶</th>
<th>形状</th>
<th>维数</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>[]</td>
<td>0-D</td>
<td>0 维张量。标量。</td>
</tr>
<tr>
<td>1</td>
<td>[D0]</td>
<td>1-D</td>
<td>形状为 [6] 的 1 维张量。</td>
</tr>
<tr>
<td>2</td>
<td>[D0, D1]</td>
<td>2-D</td>
<td>形状为 [4, 3] 的 2 维张量。</td>
</tr>
<tr>
<td>3</td>
<td>[D0, D1, D2]</td>
<td>3-D</td>
<td>形状为 [1, 2, 3] 的 3 维张量。</td>
</tr>
<tr>
<td>n</td>
<td>[D0, D1, … Dn-1]</td>
<td>n 维</td>
<td>形状为 [D0, D1, … Dn-1] 的张量。</td>
</tr>
</tbody>
</table>
</div>
<p><a href="https://www.tensorflow.org/programmers_guide/tensors#shape" target="_blank" rel="noopener">文档</a>中介绍得更详细。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># show the shape of tensor</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    scalar = tf.ones([]) <span class="comment"># a scalar / 0-D tensor :1</span></span><br><span class="line">    vector = tf.ones([<span class="number">6</span>]) <span class="comment"># a vector with 6 elements: [1,1,1 ,1,1,1]</span></span><br><span class="line">    matrix = tf.ones([<span class="number">2</span>, <span class="number">3</span>]) <span class="comment"># a matrix with 2 rows and 3 columns</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># use tf.get_shape() </span></span><br><span class="line">        print(<span class="string">"Scalar shape: "</span>,scalar.get_shape(), <span class="string">" value: "</span>, sess.run(scalar))</span><br><span class="line">        print(<span class="string">"Vector shape: "</span>,vector.get_shape(), <span class="string">" value: "</span>, sess.run(vector))</span><br><span class="line">        print(<span class="string">"Matrix shape: "</span>,matrix.get_shape(), <span class="string">" value: "</span>, sess.run(matrix))</span><br></pre></td></tr></table></figure>
<pre><code>D:\Anaconda3\Anaconda3_py36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters


Scalar shape:  ()  value:  1.0
Vector shape:  (6,)  value:  [1. 1. 1. 1. 1. 1.]
Matrix shape:  (2, 3)  value:  [[1. 1. 1.]
 [1. 1. 1.]]
</code></pre><h3 id="获取张量形状"><a href="#获取张量形状" class="headerlink" title="获取张量形状"></a>获取张量形状</h3><p>可以通过查看张量对象的<code>shape</code>属性来获取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vector.shape</span><br></pre></td></tr></table></figure>
<pre><code>TensorShape([Dimension(6)])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matrix.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>Dimension(3)
</code></pre><h3 id="获取张量的数据类型"><a href="#获取张量的数据类型" class="headerlink" title="获取张量的数据类型"></a>获取张量的数据类型</h3><p>查看张量对象的<code>dtype</code>属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matrix.dtype</span><br></pre></td></tr></table></figure>
<pre><code>tf.float32
</code></pre><h3 id="改变张量数据类型"><a href="#改变张量数据类型" class="headerlink" title="改变张量数据类型"></a>改变张量数据类型</h3><p><code>tf.cast</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c = tf.constant([<span class="number">1</span>,<span class="number">9</span>,<span class="number">8</span>,<span class="number">3</span>])</span><br><span class="line">print(c.dtype)</span><br><span class="line">f = tf.cast(c, dtype=tf.float32)</span><br><span class="line">print(f.dtype)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;dtype: &#39;int32&#39;&gt;
&lt;dtype: &#39;float32&#39;&gt;
</code></pre><h3 id="获取张量的阶"><a href="#获取张量的阶" class="headerlink" title="获取张量的阶"></a>获取张量的阶</h3><p><code>tf.rank()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.rank(scalar)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor &#39;Rank:0&#39; shape=() dtype=int32&gt;
</code></pre><h3 id="张量切片"><a href="#张量切片" class="headerlink" title="张量切片"></a>张量切片</h3><p>对于n阶张量，要访问其中某一元素，需要制定n个索引。</p>
<p><code>:</code>是Python切片语法，也意味着<strong>不要变更该维度</strong>。可以帮助访问张量的子向量，子矩阵和子张量。</p>
<h2 id="Broadcasting-广播"><a href="#Broadcasting-广播" class="headerlink" title="Broadcasting | 广播"></a>Broadcasting | 广播</h2><p>tensorflow支持广播，借鉴了Numpy中的做法，<a href="https://docs.scipy.org/doc/numpy-1.10.1/user/basics.broadcasting.html" target="_blank" rel="noopener">Numpy Broadcasting</a>.</p>
<p>数学中，相同形状的张量才能进行元素级的运算，例如相加和等于。由于广播，使得不同形状的张量运算可以像对标量进行运算一样。</p>
<p>当张量被广播时，相当于对张量进行复制，实际上并不复制，广播专门为实现性能优化而设计。</p>
<p>举例，假设你和四个小伙伴，年龄分别为[18, 17, 20, 22, 21],每年年龄+1,模拟这个过程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量加法</span></span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    <span class="comment"># method 1</span></span><br><span class="line">    ages = tf.constant([<span class="number">18</span>, <span class="number">17</span>, <span class="number">20</span>, <span class="number">22</span>, <span class="number">21</span>])</span><br><span class="line">    one = tf.constant([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    new_ages = tf.add(ages, one)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        print(new_ages.eval())</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># method 2</span></span><br><span class="line">    one_ = tf.constant(<span class="number">1</span>)</span><br><span class="line">    new_ages_ = tf.add(ages, one_)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        print(new_ages_.eval())</span><br></pre></td></tr></table></figure>
<pre><code>[19 18 21 23 22]
[19 18 21 23 22]
</code></pre><h3 id="张量变形"><a href="#张量变形" class="headerlink" title="张量变形"></a>张量变形</h3><p>可以使用<code>tf.reshape()</code>来改变张量的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">arr = np.arange(<span class="number">1</span>, <span class="number">13</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">np.random.shuffle(arr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    x = tf.constant(arr, dtype=tf.int32) <span class="comment"># create a 3x4 matrix/ 2-D tensor</span></span><br><span class="line">    reshaped_4x3_x = tf.reshape(x, [<span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">    reshaped_2x6_x = tf.reshape(x, [<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line">    reshaped_3x2x2_x = tf.reshape(x, [<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>]) <span class="comment"># reshape the rank </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        print(<span class="string">"Original matrix (3x4):"</span>)</span><br><span class="line">        print(x.eval())</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"Reshaped matrix (4x3)"</span>)</span><br><span class="line">        print(reshaped_4x3_x.eval())</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"Reshaped matrix (2x6)"</span>)</span><br><span class="line">        print(reshaped_2x6_x.eval())</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">"Reshaped matrix (3x2x2)"</span>)</span><br><span class="line">        print(reshaped_3x2x2_x.eval())</span><br></pre></td></tr></table></figure>
<pre><code>Original matrix (3x4):
[[ 9 10 11 12]
 [ 1  2  3  4]
 [ 5  6  7  8]]
Reshaped matrix (4x3)
[[ 9 10 11]
 [12  1  2]
 [ 3  4  5]
 [ 6  7  8]]
Reshaped matrix (2x6)
[[ 9 10 11 12  1  2]
 [ 3  4  5  6  7  8]]
Reshaped matrix (3x2x2)
[[[ 9 10]
  [11 12]]

 [[ 1  2]
  [ 3  4]]

 [[ 5  6]
  [ 7  8]]]
</code></pre><h2 id="变量、初始化和赋值"><a href="#变量、初始化和赋值" class="headerlink" title="变量、初始化和赋值"></a>变量、初始化和赋值</h2><p>TensorFlow变量初始化不是自动进行的，调用<code>tf.global_variables_initializer()</code>。不初始化就会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    v = tf.Variable([<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            sess.run(v)</span><br><span class="line">        <span class="keyword">except</span> tf.errors.FailedPreconditionError <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">"Caught excepted error: "</span>, e)</span><br></pre></td></tr></table></figure>
<pre><code>Caught excepted error:  Attempting to use uninitialized value Variable
     [[Node: _retval_Variable_0_0 = _Retval[T=DT_INT32, index=0, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](Variable)]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    v = tf.Variable([<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            print(sess.run(v))</span><br><span class="line">        <span class="keyword">except</span> tf.errors.FailedPreconditionError <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">"Caught excepted error: "</span>, e)</span><br></pre></td></tr></table></figure>
<pre><code>[3]
</code></pre><h3 id="assign"><a href="#assign" class="headerlink" title="assign"></a>assign</h3><p>要变更变量的值，使用<code>tf.assign()</code>指令，仅仅创建assign指令也不能起作用。和初始化一样，也需要运行赋值指令才能变更变量值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    v = tf.Variable([<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        assignment = tf.assign(v, [<span class="number">9</span>])</span><br><span class="line">        print(v.eval()) <span class="comment"># the variable has not been changed yet.</span></span><br><span class="line">        sess.run(assignment)</span><br><span class="line">        print(v.eval()) <span class="comment"># now the variable is updated</span></span><br></pre></td></tr></table></figure>
<pre><code>[3]
[9]
</code></pre><h3 id="评估张量"><a href="#评估张量" class="headerlink" title="评估张量"></a>评估张量</h3><p><code>eval()</code>。<br>方法仅在默认 tf.Session 处于活动状态时才起作用。<code>Tensor.eval()</code>会返回一个和张量内容相同的Numpy数组。</p>
<p>仅仅只有占位符的情况下无法进行评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    p = tf.placeholder(tf.float32)</span><br><span class="line">    t = p+<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># t.eval() # this will fail, since the placeholder did not give a value</span></span><br><span class="line">        print(t.eval(feed_dict=&#123;p:<span class="number">23.3</span>&#125;) )<span class="comment"># this will success, because a value is fed to the placeholder</span></span><br></pre></td></tr></table></figure>
<pre><code>24.3
</code></pre><h2 id="举例：模拟投掷两个骰子10次"><a href="#举例：模拟投掷两个骰子10次" class="headerlink" title="举例：模拟投掷两个骰子10次"></a>举例：模拟投掷两个骰子10次</h2><p>素材来自<a href="https://colab.research.google.com/notebooks/mlcc/creating_and_manipulating_tensors.ipynb?hl=zh-cn#scrollTo=iFIOcnfz_Oqw" target="_blank" rel="noopener">这里</a>.</p>
<p>模拟<a href="https://book.douban.com/subject/1082154/" target="_blank" rel="noopener">《活着》</a>中富贵儿赌钱投骰子（6个面，点数从1到6）的过程，在模拟中生成一个 <code>10x4</code> 二维张量，其中：</p>
<ul>
<li>列 <code>1</code> 和 <code>2</code> 均存储一个骰子的一次投掷值。</li>
<li>列 <code>3</code> 存储同一行中列 <code>1</code> 和 <code>2</code> 的值的总和。</li>
<li>列 <code>4</code> 表示开大开小，若列 <code>3</code> 点数大于7，开大（如用1表示）；小于等于7开小（如用0表示）。</li>
</ul>
<p>例如，第一行中可能会包含以下值：</p>
<ul>
<li>列 <code>1</code> 存储 <code>4</code></li>
<li>列 <code>2</code> 存储 <code>3</code></li>
<li>列 <code>3</code> 存储 <code>7</code></li>
<li>列 <code>4</code> 存储 <code>0</code></li>
</ul>
<p>要完成此任务，可能需要浏览 <a href="https://www.tensorflow.org/api_guides/python/array_ops" target="_blank" rel="noopener">TensorFlow 文档</a>。</p>
<p><strong>问题</strong>：<br><em>如何随机并分配值给变量？（TensorFlow不支持动态计算图）</em><br><em>如何赋值十次，循环？最后张量结果如何表示？</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import numpy as np</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment"># 使用随机均匀分布 tf.random_uniform 来模拟投掷 n 次, 不需要循环。</span></span><br><span class="line">    dice1 = tf.Variable(tf.random_uniform([<span class="number">10</span>, <span class="number">1</span>],</span><br><span class="line">                                         minval=<span class="number">1</span>,</span><br><span class="line">                                         maxval=<span class="number">7</span>,</span><br><span class="line">                                         dtype=tf.int32))</span><br><span class="line">    </span><br><span class="line">    dice2 = tf.Variable(tf.random_uniform([<span class="number">10</span>, <span class="number">1</span>],</span><br><span class="line">                                         minval=<span class="number">1</span>, </span><br><span class="line">                                         maxval=<span class="number">7</span>, </span><br><span class="line">                                         dtype=tf.int32))</span><br><span class="line">    </span><br><span class="line">    dice_sum = tf.add(dice1, dice2)</span><br><span class="line">    </span><br><span class="line">    seven = tf.constant(<span class="number">7</span>)</span><br><span class="line">    <span class="comment"># 关于TensorFlow条件控制</span></span><br><span class="line">    <span class="comment"># https://www.tensorflow.org/versions/r1.8/api_guides/python/control_flow_ops#Control_Flow_Operations</span></span><br><span class="line">    comp = tf.cast(tf.greater(dice_sum, seven), tf.int32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 tf.concat 连接向量，axis=1 水平方向连接</span></span><br><span class="line">    result = tf.concat(values=[dice1, dice2, dice_sum, comp], axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        </span><br><span class="line">        print(result.eval())</span><br></pre></td></tr></table></figure>
<pre><code>[[ 4  4  8  1]
 [ 6  1  7  0]
 [ 5  6 11  1]
 [ 4  2  6  0]
 [ 4  1  5  0]
 [ 5  3  8  1]
 [ 6  2  8  1]
 [ 4  2  6  0]
 [ 4  6 10  1]
 [ 1  3  4  0]]
</code></pre>
			
		  
		</div>

		
		  

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/13/Tensor-Control/" data-id="cjkyqovbi00096sfimh3mok6a" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/Tensor/">Tensor</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-Tensor" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/13/Tensor/">Tensor</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/13/Tensor/" class="article-date"><time datetime="2018-08-13T02:05:48.000Z" itemprop="datePublished">2018-08-13</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
			<h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>TensorFlow中的Tensor就是张量，张量是任意维的数组。常用的张量有：</p>
<ul>
<li><strong>constant（标量/常量）</strong>：零维数组，零阶张量。例如，0或一个字符串常量。</li>
<li><strong>vector（向量/矢量）</strong>：一维数组，一阶张量。例如，[0]或[0, 1, 1, 2, 3, 5].</li>
<li><strong>matrix（矩阵）</strong>:二维数组，二阶张量。例如，[[4, 9, 2], [3, 5, 7], [8, 1, 6]]</li>
</ul>
<p>在TensorFlow中，张量的创建、销毁和控制由<strong>指令</strong>完成。典型的TensorFlow代码大多数都是指令。</p>
<h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><p>TensorFlow中的图，也叫作计算图或数据流图（<del>数据库原理和软件工程中的数据流图看似百无一用</del>），是一种<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/1450" target="_blank" rel="noopener">数据结构</a>。</p>
<p>TensorFlow程序可以选择创建一张或者多张图。</p>
<p><strong><em>图的节点是指令；图的边是张量。</em></strong>（张量就像是在图中的流动，所以称为Tensor Flow）.</p>
<p>张量流经图，在每个节点由一个指令控制。<strong>一个指令的输出张量通常会变成后续指令的输入张量</strong>。</p>
<p>TensorFlow实现<strong>延迟执行模型</strong>，系统仅会根据相关节点的需求在需要时计算节点。</p>
<p><strong><em>常量和张量都是图中的一种指令</em></strong>，<strong>常量是始终会返回同一张量值的指令</strong>，<strong>变量是会返回分配给它的任何张量的指令</strong>。</p>
<p><strong><em>图必须要在TensorFlow会话Session中运行，会话存储了被会话运行的图的状态</em></strong>.</p>
<p>会话可以将图发布到多台设备上（假如程序在某个分布式计算框架上运行）。官方提供的<a href="https://www.tensorflow.org/deploy/distributed" target="_blank" rel="noopener">分布式TensorFlow文档</a>.</p>
<p> TensorFlow 提供了一个<strong>默认图</strong>。不过，我们建议您明确创建自己的 <code>Graph</code>，以便跟踪状态（例如，您可能希望在每个单元格中使用一个不同的 <code>Graph</code>）。</p>
<h3 id="举例，“海伦-秦九韶公式”三斜求积"><a href="#举例，“海伦-秦九韶公式”三斜求积" class="headerlink" title="举例，“海伦-秦九韶公式”三斜求积"></a>举例，“海伦-秦九韶公式”三斜求积</h3><script type="math/tex; mode=display">
S = \sqrt{p\left( p-a\right)\left( p-b\right)\left( p-c\right)}</script><script type="math/tex; mode=display">
p=\frac{a+b+c}{2}</script><p>相关<a href="https://www.tensorflow.org/api_docs/python/tf" target="_blank" rel="noopener">API</a>.</p>
			
			  <p class="article-more-link">
				<a class="btn btn-primary" href="2018/08/13/Tensor/#more">Read more...</a>
			  </p>
			
		  
		</div>

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/13/Tensor/" data-id="cjkyqovbe00076sfi5c3w54p6" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/Tensor/">Tensor</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-Variable" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/12/Variable/">Variable</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/12/Variable/" class="article-date"><time datetime="2018-08-12T14:35:05.000Z" itemprop="datePublished">2018-08-12</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
		  
			<!-- Table of Contents -->

			
			
			<p><a href="https://www.gushiwen.org/GuShiWen_dd05a84e30.aspx" target="_blank" rel="noopener">《庄子·天下》</a>中说“一尺之棰，日取其半，万世不竭。”使用TensorFlow来模拟这个过程。</p>
<h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量</span></span><br><span class="line">bar = tf.Variable(<span class="number">1.0</span>, name=<span class="string">"bar"</span>)</span><br><span class="line"></span><br><span class="line">two = tf.constant(<span class="number">2.0</span>) </span><br><span class="line">divided = tf.divide(bar, <span class="number">2.0</span>) <span class="comment"># 定义取半操作，这一步并没有直接计算</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将bar更新为新的长度</span></span><br><span class="line">update = tf.assign(bar, divided)</span><br></pre></td></tr></table></figure>
<h3 id="初始化变量"><a href="#初始化变量" class="headerlink" title="初始化变量"></a>初始化变量</h3><p><code>global_variables_initializer()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer() <span class="comment"># z定义了变量就必须要初始化</span></span><br></pre></td></tr></table></figure>
<h3 id="创建会话并运行"><a href="#创建会话并运行" class="headerlink" title="创建会话并运行"></a>创建会话并运行</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(<span class="string">'day %d len: '</span>%i, sess.run(bar))</span><br></pre></td></tr></table></figure>
<pre><code>day 0 len:  0.5
day 1 len:  0.25
day 2 len:  0.125
day 3 len:  0.0625
day 4 len:  0.03125
day 5 len:  0.015625
day 6 len:  0.0078125
day 7 len:  0.00390625
day 8 len:  0.001953125
day 9 len:  0.0009765625
</code></pre><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Variable就是变量，和变量相对的就是常量，常量就是不变的量，那么变量就是会变化的量。在TensorFlow中使用<code>tf.Variable()</code>来定义变量。定义了变量就必须要进行变量初始化，初始化使用<code>tf.global_variables_initializer()</code><del>tf.initialize_all_variables()</del>.变量在运行时也需依靠定义会话Session来run才行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 高斯的故事之高斯求和TensorFlow实现</span></span><br><span class="line">s = tf.Variable(<span class="number">0</span>, name=<span class="string">"sum"</span>)</span><br><span class="line">number = tf.Variable(<span class="number">0</span>, name=<span class="string">"number"</span>)</span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">new_number = tf.assign(number, tf.add(number, one))</span><br><span class="line">update = tf.assign(s, tf.add(s, new_number))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">    sess.run(update)</span><br><span class="line">print(sess.run(s))</span><br><span class="line"></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<pre><code>5050
</code></pre>
			
		  
		</div>

		
		  

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/12/Variable/" data-id="cjkyqovbr000h6sfi6k1qzlt6" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-Session" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/12/Session/">Session</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/12/Session/" class="article-date"><time datetime="2018-08-12T14:28:06.000Z" itemprop="datePublished">2018-08-12</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
		  
			<!-- Table of Contents -->

			
			
			<p>举两个例子。</p>
<h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">node = tf.constant(<span class="string">"Hello, world!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(node))</span><br><span class="line">sess.close</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> tf:</span><br><span class="line">    print(sess.run(node))</span><br></pre></td></tr></table></figure>
<pre><code>D:\Anaconda3\Anaconda3_py36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters


b&#39;Hello, world!&#39;
b&#39;Hello, world!&#39;
</code></pre><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>理解Session会话，并使用TensorFlow来解决买菜问题（矩阵乘法）。素材来自<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE3OTk2Ng==&amp;mid=2657440935&amp;idx=1&amp;sn=5b968c80fb07df3005039ca1466cb0c5&amp;chksm=bdd943b68aaecaa0420e1f3f9a576b1b0256ac7165a2fcf9b0c84080a5b0b53250418be594c8&amp;mpshare=1&amp;scene=1&amp;srcid=0812pRYblN0eJVNurGxLbC4F#rd" target="_blank" rel="noopener">《如何让11岁表妹知道矩阵乘法的本质是什么？》</a>.</p>
<p>假设买肉和菜，农贸市场一斤肉25元，一斤菜3元。<br>第一天我去农贸买了1斤肉，3斤菜，问计算用了多少钱：答：$25\times1+3\times3=34$。表示成向量乘法：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
 25&3 \\
\end{bmatrix} 
\times 
\begin{bmatrix}
 1 \\
 3 \\
\end{bmatrix} 
= 34</script><p>第二天，又去农贸市场买了2斤肉，1斤菜,则两天的花销可以表示为：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
 25&3 \\
\end{bmatrix} 
\times 
\begin{bmatrix}
 1&2 \\
 3&1 \\
\end{bmatrix} 
= 
\begin{bmatrix}
34 & 53
\end{bmatrix}</script><p>左边矩阵形状为1x2，行就表示<code>农贸市场</code>，两列分别表示<code>肉的单价和菜的单价</code>.右边矩阵，第一列表示第一天，第二列表示第二天，依次类推。第一行表示买的肉的数量，第二行表示买的菜的数量。两者进行矩阵相乘，得到的就是农贸市场，第n天，买肉和菜的总和。</p>
<p>听说王大妈哪里肉一斤20，菜一斤2元，假如这两天在王大妈哪里买菜，表示成矩阵：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
25&3\\
20&2\\
\end{bmatrix}
\begin{bmatrix}
1&2\\
3&1\\
\end{bmatrix}
=
\begin{bmatrix}
34&53\\
26&42\\
\end{bmatrix}</script><p><img src="https://mmbiz.qpic.cn/mmbiz_png/pojyAtdhQhNM8WaSdOWGtLciaHKXKs4DrOkXwemKzA7p3AuHKhht3em8mV0hXDyNQeBU6geAQIGoFa3WX0ThoZg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 创建计算图</span></span><br><span class="line"></span><br><span class="line">mat1 = tf.constant(np.array([[<span class="number">25</span>, <span class="number">3</span>], [<span class="number">20</span>, <span class="number">2</span>]]))</span><br><span class="line">mat2 = tf.constant(np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">1</span>]]))</span><br><span class="line">product = tf.matmul(mat1, mat2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话和运行</span></span><br><span class="line"><span class="comment"># method 1</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">money = sess.run(product)</span><br><span class="line">print(money)</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    money = sess.run(product)</span><br><span class="line">    print(money)</span><br></pre></td></tr></table></figure>
<pre><code>[[34 53]
 [26 42]]
[[34 53]
 [26 42]]
</code></pre>
			
		  
		</div>

		
		  

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/12/Session/" data-id="cjkyqovbk000a6sfi7hsej6no" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-MNIST-pros" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/12/MNIST-pros/">MNIST pros</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/12/MNIST-pros/" class="article-date"><time datetime="2018-08-12T02:11:47.000Z" itemprop="datePublished">2018-08-12</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
			<h3 id="运行TensorFlow的InteractiveSession-交互式会话"><a href="#运行TensorFlow的InteractiveSession-交互式会话" class="headerlink" title="运行TensorFlow的InteractiveSession/交互式会话"></a>运行TensorFlow的InteractiveSession/交互式会话</h3><p><code>sess = tf.InteractiveSession()</code>用来创建交互式的会话，其余使用同Session</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<pre><code>D:\Anaconda3\Anaconda3_py36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
</code></pre><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>和上一节类似</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># http://yann.lecun.com/exdb/mnist/  to download mnist dataset</span></span><br><span class="line"><span class="comment"># https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py</span></span><br><span class="line"><span class="comment"># to read dataset</span></span><br><span class="line"><span class="keyword">from</span> load_mnist <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line">minst_dir = <span class="string">"MNIST_data/"</span></span><br><span class="line">mnist = input_data.read_data_sets(minst_dir, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="keyword">None</span>, <span class="number">784</span>]) </span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="keyword">None</span>, <span class="number">10</span>]) <span class="comment"># true label</span></span><br></pre></td></tr></table></figure>
<h2 id="构建一个多层卷积网络"><a href="#构建一个多层卷积网络" class="headerlink" title="构建一个多层卷积网络"></a>构建一个多层卷积网络</h2><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>) <span class="comment">#  The generated values follow a normal distribution with specified mean and</span></span><br><span class="line">    <span class="comment"># standard deviation.从截断的正态分布中输出随机值，中如果x的取值在区间（μ-2σ，μ+2σ）之外则重新进行选择</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape=shape) <span class="comment"># Constant 2-D tensor populated with scalar value 0.1. 初始值为0.1，广播到shape形状的二维数组</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<h3 id="卷积和汇聚（池化）"><a href="#卷积和汇聚（池化）" class="headerlink" title="卷积和汇聚（池化）"></a>卷积和汇聚（<del>池化</del>）</h3><p><em>由于Pooling翻译成池化让人不明所以，个人习惯依据其作用，而称之为汇聚。</em></p>
<p>简单起见，卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
			
			  <p class="article-more-link">
				<a class="btn btn-primary" href="2018/08/12/MNIST-pros/#more">Read more...</a>
			  </p>
			
		  
		</div>

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/12/MNIST-pros/" data-id="cjkyqovb500036sfiswpyyv5j" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/mnist/">mnist</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-TensorFlow-tutorial-mnist-beginners" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/11/TensorFlow-tutorial-mnist-beginners/">TensorFlow tutorial mnist beginners</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/11/TensorFlow-tutorial-mnist-beginners/" class="article-date"><time datetime="2018-08-11T09:25:06.000Z" itemprop="datePublished">2018-08-11</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Machine-Learning/">Machine Learning</a> / <a class="article-category-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
		  
			<!-- Table of Contents -->

			
			
			<h1 id="从示例开始"><a href="#从示例开始" class="headerlink" title="从示例开始"></a>从示例开始</h1><h3 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 和tf配合使用</span></span><br></pre></td></tr></table></figure>
<pre><code>D:\Anaconda3\Anaconda3_py36\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
</code></pre><h3 id="构造线性模型"><a href="#构造线性模型" class="headerlink" title="构造线性模型"></a>构造线性模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate phony data | 生成假数据</span></span><br><span class="line"><span class="comment"># 实际应用应该是面对真实的数据</span></span><br><span class="line"><span class="comment"># 100个点，（2x100）</span></span><br><span class="line">x_data = np.float32(np.random.rand(<span class="number">2</span>, <span class="number">100</span>))</span><br><span class="line">y_data = np.dot([<span class="number">0.100</span>, <span class="number">0.200</span>], x_data)+<span class="number">0.300</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造线性模型</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>])) <span class="comment"># tensor of shape 1</span></span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], minval=<span class="number">-0.1</span>, maxval=<span class="number">1.0</span>)) <span class="comment"># tensor of shape 1x2 whose values are from a uniform distribution </span></span><br><span class="line"><span class="comment"># in the range of [minval, maxval)</span></span><br><span class="line">y = tf.matmul(W, x_data)+b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小化方差</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y-y_data))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line"><span class="comment"># init = tf.initialize_all_variables()</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动计算图(graph)</span></span><br><span class="line">sess = tf.Session() <span class="comment"># 创建会话</span></span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拟合平面</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">201</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step%<span class="number">20</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Step: "</span>, step, <span class="string">" W= "</span>,sess.run(W), <span class="string">" b= "</span>, sess.run(b))</span><br></pre></td></tr></table></figure>
<pre><code>Step:  0  W=  [[0.4871506  0.41139394]]  b=  [-0.09778017]
Step:  20  W=  [[0.22539519 0.34252512]]  b=  [0.15573283]
Step:  40  W=  [[0.14763923 0.26675704]]  b=  [0.23892412]
Step:  60  W=  [[0.11975352 0.22945689]]  b=  [0.2737922]
Step:  80  W=  [[0.10842387 0.21279107]]  b=  [0.2887098]
Step:  100  W=  [[0.10362242 0.20552918]]  b=  [0.2951307]
Step:  120  W=  [[0.10156146 0.20238699]]  b=  [0.29789925]
Step:  140  W=  [[0.10067356 0.2010301 ]]  b=  [0.2990936]
Step:  160  W=  [[0.10029061 0.20044449]]  b=  [0.29960892]
Step:  180  W=  [[0.10012538 0.20019177]]  b=  [0.29983127]
Step:  200  W=  [[0.10005409 0.20008272]]  b=  [0.2999272]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭会话</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<h2 id="MNIST机器学习入门"><a href="#MNIST机器学习入门" class="headerlink" title="MNIST机器学习入门"></a>MNIST机器学习入门</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># http://yann.lecun.com/exdb/mnist/  to download mnist dataset</span></span><br><span class="line"><span class="comment"># https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py</span></span><br><span class="line"><span class="comment"># to read dataset</span></span><br><span class="line"><span class="keyword">from</span> load_mnist <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line">minst_dir = <span class="string">"MNIST_data/"</span></span><br><span class="line">mnist = input_data.read_data_sets(minst_dir, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From D:\Anaconda3\Anaconda3_py36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
</code></pre><p>下载下来的数据集被分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）.</p>
<p>每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。我们把这些图片设为“xs”，把这些标签设为“ys”。训练数据集和测试数据集都包含xs和ys，比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels.</p>
<p>每一张图片包含28像素X28像素。我们可以用一个数字数组来表示这张图片：<br><img src="http://www.tensorfly.cn/tfdoc/images/MNIST-Matrix.png" alt=""></p>
<p>我们把这个数组展开成一个向量，长度是 28x28 = 784。如何展开这个数组（数字间的顺序）不重要，只要保持各个图片采用相同的方式展开。从这个角度来看，MNIST数据集的图片就是在784维向量空间里面的点, 并且拥有比较复杂的结构 (提醒: 此类数据的可视化是计算密集型的)。</p>
<p>在MNIST训练数据集中，mnist.train.images 是一个形状为 <code>[60000, 784]</code>的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。<br><img src="http://www.tensorfly.cn/tfdoc/images/mnist-train-xs.png" alt=""></p>
<p>相对应的MNIST数据集的标签是介于0到9的数字，用来描述给定图片里表示的数字。为了用于这个教程，我们使标签数据是”one-hot vectors”。 一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0。所以在此教程中，数字n将表示成一个只有在第n维度（从0开始）数字为1的10维向量。比如，标签0将表示成(<code>[1,0,0,0,0,0,0,0,0,0,0])</code>。因此， mnist.train.labels 是一个<code>[60000, 10]</code>的数字矩阵。<br><img src="http://www.tensorfly.cn/tfdoc/images/mnist-train-ys.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x represent the input</span></span><br><span class="line"><span class="comment"># None means number of images is adaptable</span></span><br><span class="line"><span class="comment"># a 28x28 image is flatten to a vector of 784</span></span><br><span class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="Softmax-regression"><a href="#Softmax-regression" class="headerlink" title="Softmax regression"></a>Softmax regression</h3><p>关于Softmax可以阅读<a href="http://www.tensorfly.cn/tfdoc/tutorials/mnist_beginners.html" target="_blank" rel="noopener">这篇</a>或者<a href="http://neuralnetworksanddeeplearning.com/chap3.html" target="_blank" rel="noopener">这篇</a>.</p>
<p><img src="http://www.tensorfly.cn/tfdoc/images/mnist6.png" alt=""></p>
<p>整个回归模型：<br><img src="http://www.tensorfly.cn/tfdoc/images/softmax-regression-scalargraph.png" alt=""><br><img src="http://www.tensorfly.cn/tfdoc/images/mnist7.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax</span></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W)+b)</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>使用交叉熵损失函数，更多介绍看<a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">这篇</a>.<br><img src="http://www.tensorfly.cn/tfdoc/images/mnist10.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意，这里的交叉熵不仅仅用来衡量单一的一对预测和真实值，而是所有100幅图片的交叉熵的总和</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gradient descent and backprapagation</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>更多优化算法点击<a href="http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#optimizers" target="_blank" rel="noopener">这里</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># init graph</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1001</span>):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        feed_dict =&#123;x: batch_xs, y_:batch_ys&#125;</span><br><span class="line">        sess.run(train_step, feed_dict)</span><br></pre></td></tr></table></figure>
<h3 id="评估我们的模型"><a href="#评估我们的模型" class="headerlink" title="评估我们的模型"></a>评估我们的模型</h3><p>tf.argmax 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值，而 tf.argmax(y_,1) 代表正确的标签，我们可以用 tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1001</span>):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        feed_dict =&#123;x: batch_xs, y_:batch_ys&#125;</span><br><span class="line">        sess.run(train_step, feed_dict)</span><br><span class="line">        <span class="keyword">if</span> step%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">            accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">            print(step, <span class="string">" train acc: "</span>, sess.run(accuracy, feed_dict))</span><br></pre></td></tr></table></figure>
<pre><code>0  train acc:  0.48
100  train acc:  0.94
200  train acc:  0.96
300  train acc:  0.93
400  train acc:  0.93
500  train acc:  0.89
600  train acc:  0.96
700  train acc:  0.94
800  train acc:  0.94
900  train acc:  0.95
1000  train acc:  0.98
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1001</span>):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        feed_dict =&#123;x: batch_xs, y_:batch_ys&#125;</span><br><span class="line">        sess.run(train_step, feed_dict)</span><br><span class="line"><span class="comment">#         if step%100==0:</span></span><br><span class="line"><span class="comment">#             correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))</span></span><br><span class="line"><span class="comment">#             accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))</span></span><br><span class="line"><span class="comment">#             print(step, " train acc: ", sess.run(accuracy, feed_dict))</span></span><br><span class="line"><span class="comment">#   # for test set</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">    print(<span class="string">"Test acc:"</span>, sess.run(accuracy, feed_dict=&#123;x:mnist.test.images, y_:mnist.test.labels&#125;))</span><br></pre></td></tr></table></figure>
<pre><code>Test acc: 0.9138
</code></pre><p>更多内容<a href="https://www.tensorflow.org/tutorials/" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文首先通过一个完整的线性回归的例子，展示了TensorFlow的基本使用，首先构造计算图graph，其中包含诸如<code>Variable</code>、<code>placeholder</code>等节点，以及节点之间的数学运算如矩阵乘法；接着对计算图初始化，<code>tf.global_variables_initializer()</code>，其中TensorFlow中很重要的就是回话session机制，通过会话来运行计算图；随后，通过会话进行训练，拟合并德大检测结果。</p>
<p>其次，使用TensorFlow完成了基于MNIST数据集，进行手写体识别的任务，并在测试集上实现了91%的正确率。<br>其完整过程，首先是准备数据，下载的数据被分为训练数据和测试数据。每种数据的基本单元都包含两部分，一部分是手写体图片，统一规格28x28，另一部分是对应的标记，来指出对应的数字，将原始图片展开向量表示，标记采用one-hot表示；<br>接着，构建softmax回归模型。使用梯度下降算法进行反向传播，最小化交叉熵损失进行训练。<br>最后，在测试机上测试，得到实验正确率指标，可以达到91%以上。</p>

			
		  
		</div>

		
		  

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/11/TensorFlow-tutorial-mnist-beginners/" data-id="cjkyqovbn000e6sfiduyu5epg" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/TensorFlow/">TensorFlow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/mnist/">mnist</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
		<article id="post-code-hightlight" class="article article-type-post" itemscope itemprop="blogPost">
	


	    <header class="article-header">
		
  
    <h1 itemprop="name">
      <a class="article-title" href="2018/08/11/code-hightlight/">Code Highlight</a>
    </h1>
  


			
		
		<div class="article-meta"> 
			
	  
		   <div class="article-datetime">
  <a href="2018/08/11/code-hightlight/" class="article-date"><time datetime="2018-08-11T04:30:03.000Z" itemprop="datePublished">2018-08-11</time></a>
</div>



			
			  
			

 
			
  <div class="article-category">
    <a class="article-category-link" href="categories/Projects/">Projects</a> / <a class="article-category-link" href="categories/Projects/hexo-theme-bootstrap-blog/">hexo-theme-bootstrap-blog</a> / <a class="article-category-link" href="categories/Projects/hexo-theme-bootstrap-blog/Tutorial/">Tutorial</a>
  </div>


	    </div>
	    </header>
	    
	  
	  <div class="article-inner">
		<div class="article-entry" itemprop="articleBody">
		  
		  
			<!-- Table of Contents -->

			
			
			<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This is some sample code to illustrate how things look!</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">import</span> Musician <span class="keyword">from</span> <span class="string">'./liverpool'</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Paul</span> <span class="keyword">extends</span> <span class="title">Musician</span> </span>&#123;</span><br><span class="line">    <span class="keyword">constructor</span>(bass) &#123;</span><br><span class="line">        <span class="keyword">super</span>(bass);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    get fullName() &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Paul McCartney'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    perform() &#123;</span><br><span class="line">        <span class="keyword">this</span>.play(<span class="keyword">this</span>.instrument);</span><br><span class="line">        <span class="keyword">this</span>.sing();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> Paul;</span><br></pre></td></tr></table></figure>
			
		  
		</div>

		
		  

		

		<footer class="article-footer">
		  <a data-url="https://www.blankspace.cn/2018/08/11/code-hightlight/" data-id="cjkyqovbx000n6sfis4zahva9" class="article-share-link">
			<i class="fa fa-share"></i> Share
		  </a>
		  
		  
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/Code/">Code</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/Hexo/">Hexo</a></li></ul>


		</footer>
	  </div>
	  
	</article>

	



		


	  
	
	
	
	  <div id="page-nav">
		<nav><ul class="pagination"><li class="disabled"><span class="page-prev"><i class="fa fa-chevron-left"></i> Prev</a></li><li class="active"><span class="page-number">1</span></li><li><a class="page-number" href="/page/2/">2</a></li><li><a class="page-number" href="/page/3/">3</a></li><li><a class="page-next" rel="next" href="/page/2/">Next <i class="fa fa-chevron-right"></i></a></li></ul></nav>
	  </div>
	

    </div>
    <div class="col-sm-3 col-sm-offset-0 blog-sidebar">
	  
  <div class="sidebar-module sidebar-module-inset">
<h4>Translate</h4>
<div id="google_translate_element"></div><script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({pageLanguage: 'en', layout: google.translate.TranslateElement.InlineLayout.SIMPLE, autoDisplay: false, multilanguagePage: true, gaTrack: true, gaId: 'UA-122713769-1'}, 'google_translate_element');
}
</script><script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
        </div>

  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="2018/08/14/MathJax-basic-tutorial/">MathJax basic tutorial and quick reference</a>
        </li>
      
        <li>
          <a href="2018/08/13/activation-function/">Activation function</a>
        </li>
      
        <li>
          <a href="2018/08/13/placeholder/">Placeholder</a>
        </li>
      
        <li>
          <a href="2018/08/13/Tensor-Control/">Tensor Control</a>
        </li>
      
        <li>
          <a href="2018/08/13/Tensor/">Tensor</a>
        </li>
      
    </ul>
  </div>


  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <a target="_blank" href="about"> <img  id="avatar-logo" width=100%  alt="WizardLQ" src="http://www.gravatar.com/avatar/d14c7c7faf5e1f3001c070f3141d698f?s=512"> </a> <p>Hi,我是一名魔法师，目前自动化硕士在读.<em><strong>“科技是普通人眼中的魔法”</strong></em>.之前有在<a href="https://blog.csdn.net/icurious">CSDN</a>、博客园、简书等写过博客. 写博客一方面能够帮助自己查缺补漏，总结和反思，同时还能被他人看到和甚至对他人产生帮助，十分有意义.若有读者在阅读中发现问题或者有什么意见或者建议，欢迎联系我的邮箱， 或者在博客或者博文中留言，我会尽量回复.<em>(由于平时比较忙<s>我比较懒</s>，有些文章可能需要很长时间才能完善.)</em><br/><strong>你通过下列方式联系到我：</strong></br> <i class="iconfont">&#xe66e;</i><em>Email: <a>liuqidev#gmail.com</a></em>| <i class="iconfont">&#xe64f;</i><a href="https://blog.csdn.net/icurious">CSDN</a>| <i class="iconfont">&#xe655;</i><a href=“https://weibo.com/liuqidev/”>Weibo</a>| <i class="iconfont">&#xe6e1;</i><a href="https://space.bilibili.com/20204877/#/">BiliBili</a> </p>

</div>




  
  <div class="sidebar-module">
    <h4>Categories</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Languages/">Languages</a><span class="sidebar-module-list-count">1</span><ul class="sidebar-module-list-child"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Languages/Python/">Python</a><span class="sidebar-module-list-count">1</span><ul class="sidebar-module-list-child"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Languages/Python/Numpy/">Numpy</a><span class="sidebar-module-list-count">1</span></li></ul></li></ul></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Machine-Learning/">Machine Learning</a><span class="sidebar-module-list-count">8</span><ul class="sidebar-module-list-child"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Machine-Learning/TensorFlow/">TensorFlow</a><span class="sidebar-module-list-count">8</span></li></ul></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Math/">Math</a><span class="sidebar-module-list-count">1</span><ul class="sidebar-module-list-child"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Math/MathJax/">MathJax</a><span class="sidebar-module-list-count">1</span></li></ul></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Projects/">Projects</a><span class="sidebar-module-list-count">19</span><ul class="sidebar-module-list-child"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Projects/hexo-theme-bootstrap-blog/">hexo-theme-bootstrap-blog</a><span class="sidebar-module-list-count">19</span><ul class="sidebar-module-list-child"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="categories/Projects/hexo-theme-bootstrap-blog/Tutorial/">Tutorial</a><span class="sidebar-module-list-count">19</span></li></ul></li></ul></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="archives/2018/08/">八月 2018</a><span class="sidebar-module-list-count">28</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="archives/2013/12/">十二月 2013</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  <section>
	<h4>Search</h4>
	<div class="sidebar-module sidebar-module-inset">
	<script>
	  (function() {
		var cx = '002232087062382108730:u5mxrltjsyw';
		var gcse = document.createElement('script');
		gcse.type = 'text/javascript';
		gcse.async = true;
		gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
		var s = document.getElementsByTagName('script')[0];
		s.parentNode.insertBefore(gcse, s);
	  })();
	</script>
	<gcse:search></gcse:search>
	
	<div id="site_search">
	<input type="text" placeholder="Search in the site" class="form-control" id="local-search-input" results="0"/>
	<div id="local-search-result">
	</div>
	</div>
	

	
</section>


	</div>
		
</div>
	
	
</div>




	
<footer class="blog-footer">  
  <div class="container">

    <div id="footer-info" class="text-center">
        &copy; 2018. <em>All Rights Reserved by <a href="https://github.com/liuqidev" target="_blank">liuqidev</a></em><br>	
	</div>
	<div > 

	  </links>
  </div>

  <p>
  	<span id="busuanzi_container_site_uv">
  	    <span id="busuanzi_value_site_uv"></span> visitors  |  
  	</span>

      <span id="busuanzi_container_site_pv">
         <span id="busuanzi_value_site_pv"></span> visits
      </span>
	  
  </p>
  
  <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</footer>


	




	


<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?70a761ba668ab8571ac79968adcf6078";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>








	
<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'Opps...●﹏●';
            clearTimeout(titleTime);
        } else {
            document.title = 'o(∩_∩)o Welcome!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



 <!-- jQuery library -->
 <script src="js/jquery-slim.min.js"></script>
 <!-- Latest compiled and minified JavaScript -->
 <script src="js/bootstrap.min.js"></script>
	 <!--Back to top-->
<body id="top">
<p id="back-to-top"><a href="#top"><span></span></a></p>
</body>

<script>
$(document).ready(function() {
    //首先将#back-to-top隐藏
    $("#back-to-top").hide();
 
    //当滚动条的位置处于距顶部3600像素以下时，跳转链接出现，否则消失
    $(function() {
        $(window).scroll(function() {
            if ($(window).scrollTop() > 3600) {
                $("#back-to-top").fadeIn(1500);
            }
            else {
                $("#back-to-top").fadeOut(1500);
            }
        });
        //当点击跳转链接后，回到页面顶部位置
        $("#back-to-top").click(function() {
            $('body,html').animate({
                scrollTop: 0
            },
            500);
			
            return false;
        });
    });
});
</script>

</body>
</html>
